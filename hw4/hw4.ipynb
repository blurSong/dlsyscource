{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 4\n",
    "\n",
    "In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset.\n",
    "\n",
    "As always, we will start by copying this notebook and getting the starting code.\n",
    "Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw4.git\n",
    "%cd /content/drive/MyDrive/10714/hw4\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 9.4.0\n",
      "-- The CXX compiler identification is GNU 9.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Python: /bin/python3.8 (found version \"3.8.10\") found components: Development Interpreter Development.Module Development.Embed \n",
      "-- Performing Test HAS_FLTO\n",
      "-- Performing Test HAS_FLTO - Success\n",
      "-- Found pybind11: /usr/local/include (found version \"2.10.1\")\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "-- Looking for pthread_create in pthreads\n",
      "-- Looking for pthread_create in pthreads - not found\n",
      "-- Looking for pthread_create in pthread\n",
      "-- Looking for pthread_create in pthread - found\n",
      "-- Found Threads: TRUE  \n",
      "-- Found CUDA: /usr/local/cuda-11.8 (found version \"11.8\") \n",
      "-- Found cuda, building cuda backend\n",
      "Tue Dec 27 17:23:39 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.53.01    Driver Version: 526.67       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A300...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8     9W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.6\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /mnt/c/SRCs/dlsyscourse/hw4/build\n",
      "make[1]: Entering directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "make[2]: Entering directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "make[3]: Entering directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "make[3]: Leaving directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "make[3]: Entering directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /mnt/c/SRCs/dlsyscourse/hw4/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "[ 75%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n",
      "make[3]: Leaving directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "make[3]: Entering directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX shared module /mnt/c/SRCs/dlsyscourse/hw4/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-38-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n",
      "make[1]: Leaving directory '/mnt/c/SRCs/dlsyscourse/hw4/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets you will be using for this assignment\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ND Backend [10 pts]\n",
    "\n",
    "Fill in the following classes in `python/needle/ops.py`:\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `Transpose`\n",
    "- `Reshape`\n",
    "- `BroadcastTo`\n",
    "- `Summation`\n",
    "- `MatMul`\n",
    "- `Negate`\n",
    "- `Log`\n",
    "- `Exp`\n",
    "- `ReLU`\n",
    "- `LogSumExp`\n",
    "- `Tanh` (new)\n",
    "- `Stack` (new)\n",
    "- `Split` (new)\n",
    "\n",
    "Note that for most of these, you already wrote the solutions in the previous homework and you should not need to change your previous solution, however `TanhOp`, `Stack`, and `Split` are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n",
    "\n",
    "**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n",
    "\n",
    "**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0 -- /bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/d/SRCs/dlsyscourse/hw4\n",
      "collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\u001b[1m\u001b[1m\n",
      "\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  0%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  1%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m        [  2%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m      [  3%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  4%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  5%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  6%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  7%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [  8%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  9%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 10%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 11%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 12%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 13%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 14%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 15%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 16%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 17%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 18%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 19%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 20%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 21%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m               [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m            [ 22%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 23%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 24%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 25%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 26%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 27%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 28%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 29%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 30%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 31%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 32%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 33%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 34%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_power[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 35%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 36%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 37%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_log[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 38%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 39%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 40%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 41%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 42%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 43%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 44%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 45%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 46%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 47%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 48%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 49%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 51%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 52%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 53%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 54%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 55%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 56%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 57%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 58%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 59%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 60%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 61%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 62%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m         [ 63%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m            [ 64%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m            [ 65%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m            [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 66%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 67%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 68%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 69%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 71%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 72%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 74%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 75%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 76%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m      [ 80%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m      [ 81%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 82%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 83%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 84%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 85%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 86%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 87%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 88%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 89%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 90%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m       [ 91%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 92%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 93%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m         [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 94%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 95%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 96%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 97%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 98%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m           [ 99%]\u001b[0m\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m           [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-1.5997132]]])\n",
      "_A         = array([[[-1.5997132]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[-1.5997132]]]),)\n",
      "        c          = array([[[1.62337831]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc80842dee0>\n",
      "        f1         = -1.496144229321173\n",
      "        f2         = -1.4961490673655593\n",
      "        i          = 0\n",
      "        j          = 0\n",
      "        kwargs     = {}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[0.24190222]]])]\n",
      "        out        = needle.Tensor([[[-0.9216254]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[[-0.9216254]]])\n",
      "        out_grad   = needle.Tensor([[[1.6233783]]])\n",
      "        self       = <needle.ops.Tanh object at 0x7fc78f9f6d30>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc78f9f6d30>\n",
      "out_grad = needle.Tensor([[[1.6233783]]])\n",
      "node = needle.Tensor([[[-0.9216254]]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        (\u001b[96minput\u001b[39;49;00m,) = node.inputs\n",
      ">       inter = \u001b[96minput\u001b[39;49;00m.tanh()\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'Tensor' object has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "input      = needle.Tensor([[[-1.5997132]]])\n",
      "node       = needle.Tensor([[[-0.9216254]]])\n",
      "out_grad   = needle.Tensor([[[1.6233783]]])\n",
      "self       = <needle.ops.Tanh object at 0x7fc78f9f6d30>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:450: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_tanh_backward[cpu-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[-0.58456963  0.86539763  0.8150671   0.52470696 -2.4929152\n",
      "    0.28299397]\n",
      "  [ 0.8119     -1.423022  ...0.8650645  -1.3442827\n",
      "    0.07837614]\n",
      "  [-0.39979824 -0.046556   -1.3993185   2.621195   -0.13064201\n",
      "   -1.2775043 ]]])\n",
      "_A         = array([[[-0.58456963,  0.86539763,  0.8150671 ,  0.52470696,\n",
      "         -2.4929152 ,  0.28299394],\n",
      "        [ 0.8119    ,...614],\n",
      "        [-0.39979827, -0.04655599, -1.3993185 ,  2.621195  ,\n",
      "         -0.13064201, -1.2775043 ]]], dtype=float32)\n",
      "device     = cpu()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[-0.58456963  0.86539763  0.8150671   0.52470696 -2.4929152\n",
      "    0.28299397]\n",
      "  [ 0.8119     -1.423022 ...8650645  -1.3442827\n",
      "    0.07837614]\n",
      "  [-0.39979824 -0.046556   -1.3993185   2.621195   -0.13064201\n",
      "   -1.2775043 ]]]),)\n",
      "        c          = array([[[-0.71145286,  0.96028833, -0.01700081,  0.18537076,\n",
      "         -0.92995867,  1.28713895],\n",
      "        [-0.69594262,...86222,  0.24258502],\n",
      "        [-0.03949696, -0.93104237, -0.55855963, -1.24720461,\n",
      "         -1.11490311, -0.52351761]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc80842dee0>\n",
      "        f1         = -1.1420759444360495\n",
      "        f2         = -1.1420731360687344\n",
      "        i          = 0\n",
      "        j          = 119\n",
      "        kwargs     = {}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.51735192,  0.48938187, -0.00937328,  0.14225534,\n",
      "         -0.02494344,  1.18915162],\n",
      "        [-0.38370307...1642,  0.24110706],\n",
      "        [-0.03384166, -0.92918519, -0.12151853, -0.02601872,\n",
      "         -1.09565049, -0.14041837]]])]\n",
      "        out        = needle.Tensor([[[-0.5259788   0.69902825  0.67237604  0.48132467 -0.98642457\n",
      "    0.27567378]\n",
      "  [ 0.6706371  -0.8902277....69885784 -0.87269706\n",
      "    0.07821605]\n",
      "  [-0.37977633 -0.04652238 -0.88520426  0.98948044 -0.12990381\n",
      "   -0.85581857]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[[-0.5259788   0.69902825  0.67237604  0.48132467 -0.98642457\n",
      "    0.27567378]\n",
      "  [ 0.6706371  -0.8902277....69885784 -0.87269706\n",
      "    0.07821605]\n",
      "  [-0.37977633 -0.04652238 -0.88520426  0.98948044 -0.12990381\n",
      "   -0.85581857]]])\n",
      "        out_grad   = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "        self       = <needle.ops.Tanh object at 0x7fc781462640>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc781462640>\n",
      "out_grad = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "node = needle.Tensor([[[-0.5259788   0.69902825  0.67237604  0.48132467 -0.98642457\n",
      "    0.27567378]\n",
      "  [ 0.6706371  -0.8902277....69885784 -0.87269706\n",
      "    0.07821605]\n",
      "  [-0.37977633 -0.04652238 -0.88520426  0.98948044 -0.12990381\n",
      "   -0.85581857]]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        (\u001b[96minput\u001b[39;49;00m,) = node.inputs\n",
      ">       inter = \u001b[96minput\u001b[39;49;00m.tanh()\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'Tensor' object has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "input      = needle.Tensor([[[-0.58456963  0.86539763  0.8150671   0.52470696 -2.4929152\n",
      "    0.28299397]\n",
      "  [ 0.8119     -1.423022  ...0.8650645  -1.3442827\n",
      "    0.07837614]\n",
      "  [-0.39979824 -0.046556   -1.3993185   2.621195   -0.13064201\n",
      "   -1.2775043 ]]])\n",
      "node       = needle.Tensor([[[-0.5259788   0.69902825  0.67237604  0.48132467 -0.98642457\n",
      "    0.27567378]\n",
      "  [ 0.6706371  -0.8902277....69885784 -0.87269706\n",
      "    0.07821605]\n",
      "  [-0.37977633 -0.04652238 -0.88520426  0.98948044 -0.12990381\n",
      "   -0.85581857]]])\n",
      "out_grad   = needle.Tensor([[[-0.71145284  0.96028835 -0.01700081  0.18537076 -0.92995864\n",
      "    1.2871389 ]\n",
      "  [-0.69594264 -0.6974334... 0.02099861  1.3598622\n",
      "    0.24258502]\n",
      "  [-0.03949696 -0.9310424  -0.55855966 -1.2472047  -1.1149031\n",
      "   -0.5235176 ]]])\n",
      "self       = <needle.ops.Tanh object at 0x7fc781462640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:450: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[0.90795034]]])\n",
      "_A         = array([[[0.90795034]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[0.90795034]]]),)\n",
      "        c          = array([[[1.19146721]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc80842dee0>\n",
      "        f1         = 0.8580373732455918\n",
      "        f2         = 0.8580258684949064\n",
      "        i          = 0\n",
      "        j          = 0\n",
      "        kwargs     = {}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[0.57523753]]])]\n",
      "        out        = needle.Tensor([[[0.720147]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[[0.720147]]])\n",
      "        out_grad   = needle.Tensor([[[1.1914672]]])\n",
      "        self       = <needle.ops.Tanh object at 0x7fc7813125b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc7813125b0>\n",
      "out_grad = needle.Tensor([[[1.1914672]]]), node = needle.Tensor([[[0.720147]]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        (\u001b[96minput\u001b[39;49;00m,) = node.inputs\n",
      ">       inter = \u001b[96minput\u001b[39;49;00m.tanh()\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'Tensor' object has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "input      = needle.Tensor([[[0.90795034]]])\n",
      "node       = needle.Tensor([[[0.720147]]])\n",
      "out_grad   = needle.Tensor([[[1.1914672]]])\n",
      "self       = <needle.ops.Tanh object at 0x7fc7813125b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:450: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.tanh, A)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.6276671   1.7709465   2.5802875   1.0994289  -0.2560744\n",
      "   -0.30351135]\n",
      "  [-1.2755342   1.4340128 ...0.41443744  0.13385125\n",
      "    0.6493112 ]\n",
      "  [-0.8847228  -1.9263881  -1.6778411   0.45559204  0.2922058\n",
      "    1.1388148 ]]])\n",
      "_A         = array([[[ 0.6276671 ,  1.7709465 ,  2.5802875 ,  1.0994289 ,\n",
      "         -0.25607443, -0.30351138],\n",
      "        [-1.2755342 ,...12 ],\n",
      "        [-0.8847228 , -1.9263881 , -1.6778411 ,  0.455592  ,\n",
      "          0.29220578,  1.1388148 ]]], dtype=float32)\n",
      "device     = cuda()\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:142: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[ 0.6276671   1.7709465   2.5802875   1.0994289  -0.2560744\n",
      "   -0.30351135]\n",
      "  [-1.2755342   1.4340128...41443744  0.13385125\n",
      "    0.6493112 ]\n",
      "  [-0.8847228  -1.9263881  -1.6778411   0.45559204  0.2922058\n",
      "    1.1388148 ]]]),)\n",
      "        c          = array([[[-0.95230234, -0.63597503, -0.19050878,  1.35308429,\n",
      "         -2.45410143, -0.07941442],\n",
      "        [ 0.10208082,...30038, -0.55089105],\n",
      "        [ 0.04892026,  2.01325595,  0.27848269,  0.11210785,\n",
      "         -0.23827138,  1.09982379]]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function tanh at 0x7fc80842dee0>\n",
      "        f1         = -8.544052887017726\n",
      "        f2         = -8.544060294688222\n",
      "        i          = 0\n",
      "        j          = 119\n",
      "        kwargs     = {}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-0.65843506, -0.07012807, -0.00454208,  0.48793316,\n",
      "         -2.30018765, -0.0725404 ],\n",
      "        [ 0.02738021...7402, -0.37104302],\n",
      "        [ 0.02449335,  0.1619992 ,  0.0365175 ,  0.09171249,\n",
      "         -0.2190671 ,  0.37038352]]])]\n",
      "        out        = needle.Tensor([[[ 0.5564438   0.94371307  0.9885887   0.8002938  -0.25062016\n",
      "   -0.29452267]\n",
      "  [-0.8552905   0.892486 ...0.39223394  0.13305758\n",
      "    0.5712061 ]\n",
      "  [-0.7087774  -0.9584404  -0.9325808   0.4264848   0.2841637\n",
      "    0.8140146 ]]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[[ 0.5564438   0.94371307  0.9885887   0.8002938  -0.25062016\n",
      "   -0.29452267]\n",
      "  [-0.8552905   0.892486 ...0.39223394  0.13305758\n",
      "    0.5712061 ]\n",
      "  [-0.7087774  -0.9584404  -0.9325808   0.4264848   0.2841637\n",
      "    0.8140146 ]]])\n",
      "        out_grad   = needle.Tensor([[[-0.95230234 -0.63597506 -0.19050878  1.3530843  -2.4541013\n",
      "   -0.07941442]\n",
      "  [ 0.10208081 -0.42133588...0.34838298 -0.7833004\n",
      "   -0.55089104]\n",
      "  [ 0.04892026  2.0132558   0.2784827   0.11210785 -0.23827139\n",
      "    1.0998238 ]]])\n",
      "        self       = <needle.ops.Tanh object at 0x7fc78136a880>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Tanh object at 0x7fc78136a880>\n",
      "out_grad = needle.Tensor([[[-0.95230234 -0.63597506 -0.19050878  1.3530843  -2.4541013\n",
      "   -0.07941442]\n",
      "  [ 0.10208081 -0.42133588...0.34838298 -0.7833004\n",
      "   -0.55089104]\n",
      "  [ 0.04892026  2.0132558   0.2784827   0.11210785 -0.23827139\n",
      "    1.0998238 ]]])\n",
      "node = needle.Tensor([[[ 0.5564438   0.94371307  0.9885887   0.8002938  -0.25062016\n",
      "   -0.29452267]\n",
      "  [-0.8552905   0.892486 ...0.39223394  0.13305758\n",
      "    0.5712061 ]\n",
      "  [-0.7087774  -0.9584404  -0.9325808   0.4264848   0.2841637\n",
      "    0.8140146 ]]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        (\u001b[96minput\u001b[39;49;00m,) = node.inputs\n",
      ">       inter = \u001b[96minput\u001b[39;49;00m.tanh()\n",
      "\u001b[1m\u001b[31mE       AttributeError: 'Tensor' object has no attribute 'tanh'\u001b[0m\n",
      "\n",
      "input      = needle.Tensor([[[ 0.6276671   1.7709465   2.5802875   1.0994289  -0.2560744\n",
      "   -0.30351135]\n",
      "  [-1.2755342   1.4340128 ...0.41443744  0.13385125\n",
      "    0.6493112 ]\n",
      "  [-0.8847228  -1.9263881  -1.6778411   0.45559204  0.2922058\n",
      "    1.1388148 ]]])\n",
      "node       = needle.Tensor([[[ 0.5564438   0.94371307  0.9885887   0.8002938  -0.25062016\n",
      "   -0.29452267]\n",
      "  [-0.8552905   0.892486 ...0.39223394  0.13305758\n",
      "    0.5712061 ]\n",
      "  [-0.7087774  -0.9584404  -0.9325808   0.4264848   0.2841637\n",
      "    0.8140146 ]]])\n",
      "out_grad   = needle.Tensor([[[-0.95230234 -0.63597506 -0.19050878  1.3530843  -2.4541013\n",
      "   -0.07941442]\n",
      "  [ 0.10208081 -0.42133588...0.34838298 -0.7833004\n",
      "   -0.55089104]\n",
      "  [ 0.04892026  2.0132558   0.2784827   0.11210785 -0.23827139\n",
      "    1.0998238 ]]])\n",
      "self       = <needle.ops.Tanh object at 0x7fc78136a880>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:450: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.07136466  0.62180406  0.64916927  1.0328673  -0.82423985]\n",
      " [-0.09091089  0.3844313   0.36341754 -0...379   1.2153631  -1.8787898  -0.4840601  -1.4458045 ]\n",
      " [ 0.1112728  -0.8710335   1.7364272   0.6129948   0.6366247 ]])]\n",
      "A_t        = [tensor([[-0.0714,  0.6218,  0.6492,  1.0329, -0.8242],\n",
      "        [-0.0909,  0.3844,  0.3634, -0.8576, -0.0482],\n",
      "       ....7654],\n",
      "        [-1.0452,  1.2154, -1.8788, -0.4841, -1.4458],\n",
      "        [ 0.1113, -0.8710,  1.7364,  0.6130,  0.6366]])]\n",
      "_A         = [array([[-0.07136466,  0.62180406,  0.64916927,  1.0328673 , -0.82423985],\n",
      "       [-0.09091089,  0.3844313 ,  0.363417...840601 , -1.4458045 ],\n",
      "       [ 0.1112728 , -0.8710335 ,  1.7364272 ,  0.6129948 ,  0.6366247 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.07136466  0.62180406  0.64916927  1.0328673  -0.82423985]\n",
      " [-0.09091089  0.3844313   0.36341754 -0...379   1.2153631  -1.8787898  -0.4840601  -1.4458045 ]\n",
      " [ 0.1112728  -0.8710335   1.7364272   0.6129948   0.6366247 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.07136466  0.62180406  0.64916927  1.0328673  -0.82423985]\n",
      " [-0.09091089  0.3844...   1.2153631  -1.8787898  -0.4840601  -1.4458045 ]\n",
      " [ 0.1112728  -0.8710335   1.7364272   0.6129948   0.6366247 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc781359670>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.07136466  0.62180406  0.64916927  1.0328673  -0.82423985]\n",
      " [-0.09091089  0.3844...   1.2153631  -1.8787898  -0.4840601  -1.4458045 ]\n",
      " [ 0.1112728  -0.8710335   1.7364272   0.6129948   0.6366247 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc781359670>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781359760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781359760>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-0.07136466  0.62180406  0.64916927  1.0328673  -0.82423985]\n",
      " [-0.09091089  0.3844313   0.36341754 -0.85764...  -1.8787898  -0.4840601  -1.4458045 ]\n",
      " [ 0.1112728  -0.8710335   1.7364272   0.6129948   0.6366247 ]], device=cpu()),)\n",
      "        array_num  = 1\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc781359670>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cpu()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cpu()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cpu(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc781359850>\n",
      "device     = cpu()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.17882441  1.4526075   0.06863618 -0.0180711  -0.14859328]\n",
      " [-1.0338513  -1.0570263  -0.2118788  -0...055  -2.045614    0.23107018  1.2565722  -0.6088181 ]\n",
      " [-0.48849607 -0.38427642  0.2980011   0.04987274  1.2440314 ]])]\n",
      "A_t        = [tensor([[-0.1788,  1.4526,  0.0686, -0.0181, -0.1486],\n",
      "        [-1.0339, -1.0570, -0.2119, -0.0361,  1.2204],\n",
      "       ....2356],\n",
      "        [ 0.2066, -2.0456,  0.2311,  1.2566, -0.6088],\n",
      "        [-0.4885, -0.3843,  0.2980,  0.0499,  1.2440]])]\n",
      "_A         = [array([[-0.17882441,  1.4526075 ,  0.06863618, -0.0180711 , -0.14859328],\n",
      "       [-1.0338513 , -1.0570263 , -0.211878...565722 , -0.6088181 ],\n",
      "       [-0.48849607, -0.38427642,  0.2980011 ,  0.04987274,  1.2440314 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.17882441  1.4526075   0.06863618 -0.0180711  -0.14859328]\n",
      " [-1.0338513  -1.0570263  -0.2118788  -0...055  -2.045614    0.23107018  1.2565722  -0.6088181 ]\n",
      " [-0.48849607 -0.38427642  0.2980011   0.04987274  1.2440314 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.17882441  1.4526075   0.06863618 -0.0180711  -0.14859328]\n",
      " [-1.0338513  -1.0570...5  -2.045614    0.23107018  1.2565722  -0.6088181 ]\n",
      " [-0.48849607 -0.38427642  0.2980011   0.04987274  1.2440314 ]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812cd880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.17882441  1.4526075   0.06863618 -0.0180711  -0.14859328]\n",
      " [-1.0338513  -1.0570...5  -2.045614    0.23107018  1.2565722  -0.6088181 ]\n",
      " [-0.48849607 -0.38427642  0.2980011   0.04987274  1.2440314 ]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7812cd880>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812cd9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812cd9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-0.17882441  1.4526075   0.06863618 -0.0180711  -0.14859328]\n",
      " [-1.0338513  -1.0570263  -0.2118788  -0.03613...    0.23107018  1.2565722  -0.6088181 ]\n",
      " [-0.48849607 -0.38427642  0.2980011   0.04987274  1.2440314 ]], device=cpu()))\n",
      "        array_num  = 2\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812cd880>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cpu()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cpu()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cpu(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812cda90>\n",
      "device     = cpu()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m__________________________ test_stack[cpu-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[[-1.33112311e+00  1.83877885e-01 -7.14379311e-01 -1.12926376e+00\n",
      "   -1.07434380e+00  6.78442299e-01  ...01]\n",
      "  [-5.3036362e-01 -1.2659273e+00 -1.3819758e+00 -8.8034773e-01\n",
      "    1.9707146e+00  5.0163239e-01 -1.3402538e-01]]])]\n",
      "A_t        = [tensor([[[-1.3311e+00,  1.8388e-01, -7.1438e-01, -1.1293e+00, -1.0743e+00,\n",
      "           6.7844e-01,  4.6458e-01],\n",
      "     ...01],\n",
      "         [-5.3036e-01, -1.2659e+00, -1.3820e+00, -8.8035e-01,  1.9707e+00,\n",
      "           5.0163e-01, -1.3403e-01]]])]\n",
      "_A         = [array([[[-1.33112311e+00,  1.83877885e-01, -7.14379311e-01,\n",
      "         -1.12926376e+00, -1.07434380e+00,  6.78442299e-0...659273e+00, -1.3819758e+00, -8.8034773e-01,\n",
      "          1.9707146e+00,  5.0163239e-01, -1.3402538e-01]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-1.33112311e+00  1.83877885e-01 -7.14379311e-01 -1.12926376e+00\n",
      "   -1.07434380e+00  6.78442299e-01  ...01]\n",
      "  [-5.3036362e-01 -1.2659273e+00 -1.3819758e+00 -8.8034773e-01\n",
      "    1.9707146e+00  5.0163239e-01 -1.3402538e-01]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-1.33112311e+00  1.83877885e-01 -7.14379311e-01 -1.12926376e+00\n",
      "   -1.07434380e+0...]\n",
      "  [-5.3036362e-01 -1.2659273e+00 -1.3819758e+00 -8.8034773e-01\n",
      "    1.9707146e+00  5.0163239e-01 -1.3402538e-01]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812f7a30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.33112311e+00  1.83877885e-01 -7.14379311e-01 -1.12926376e+00\n",
      "   -1.07434380e+0...]\n",
      "  [-5.3036362e-01 -1.2659273e+00 -1.3819758e+00 -8.8034773e-01\n",
      "    1.9707146e+00  5.0163239e-01 -1.3402538e-01]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7812f7a30>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812f7d00>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812f7d00>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[[-1.33112311e+00  1.83877885e-01 -7.14379311e-01 -1.12926376e+00\n",
      "   -1.07434380e+00  6.78442299e-01  4.6458...362e-01 -1.2659273e+00 -1.3819758e+00 -8.8034773e-01\n",
      "    1.9707146e+00  5.0163239e-01 -1.3402538e-01]]], device=cpu()))\n",
      "        array_num  = 5\n",
      "        array_shape = [1, 5, 7]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812f7a30>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cpu()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cpu()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cpu(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812f7bb0>\n",
      "device     = cpu()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape0-0-1] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.01668357  1.1806753  -0.57011104 -0.16424966  0.1057056 ]\n",
      " [-0.01159221  0.06366155  0.31758705 -0...4927 -0.19975884 -1.3304735  -0.11766437 -1.019091  ]\n",
      " [ 0.01301353  0.40394965  1.6848668  -0.09611785 -0.1998671 ]])]\n",
      "A_t        = [tensor([[-0.0167,  1.1807, -0.5701, -0.1642,  0.1057],\n",
      "        [-0.0116,  0.0637,  0.3176, -0.9027, -0.7916],\n",
      "       ....3726],\n",
      "        [ 0.8278, -0.1998, -1.3305, -0.1177, -1.0191],\n",
      "        [ 0.0130,  0.4039,  1.6849, -0.0961, -0.1999]])]\n",
      "_A         = [array([[-0.01668357,  1.1806753 , -0.57011104, -0.16424966,  0.1057056 ],\n",
      "       [-0.01159221,  0.06366155,  0.317587...1766437, -1.019091  ],\n",
      "       [ 0.01301353,  0.40394965,  1.6848668 , -0.09611785, -0.1998671 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.01668357  1.1806753  -0.57011104 -0.16424966  0.1057056 ]\n",
      " [-0.01159221  0.06366155  0.31758705 -0...4927 -0.19975884 -1.3304735  -0.11766437 -1.019091  ]\n",
      " [ 0.01301353  0.40394965  1.6848668  -0.09611785 -0.1998671 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.01668357  1.1806753  -0.57011104 -0.16424966  0.1057056 ]\n",
      " [-0.01159221  0.0636...7 -0.19975884 -1.3304735  -0.11766437 -1.019091  ]\n",
      " [ 0.01301353  0.40394965  1.6848668  -0.09611785 -0.1998671 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812c4e50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.01668357  1.1806753  -0.57011104 -0.16424966  0.1057056 ]\n",
      " [-0.01159221  0.0636...7 -0.19975884 -1.3304735  -0.11766437 -1.019091  ]\n",
      " [ 0.01301353  0.40394965  1.6848668  -0.09611785 -0.1998671 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7812c4e50>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812c4df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812c4df0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-0.01668357  1.1806753  -0.57011104 -0.16424966  0.1057056 ]\n",
      " [-0.01159221  0.06366155  0.31758705 -0.90268... -1.3304735  -0.11766437 -1.019091  ]\n",
      " [ 0.01301353  0.40394965  1.6848668  -0.09611785 -0.1998671 ]], device=cuda()),)\n",
      "        array_num  = 1\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812c4e50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cuda()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812c4fa0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[-0.4484034   1.0540462   0.03045906 -0.4768823   1.5634812 ]\n",
      " [-1.0746397   0.9731278  -1.0777118  -1...3593  0.29344648  0.64067996  0.24550702 -0.8840323 ]\n",
      " [ 2.7280824   3.322596   -0.28550744  0.32897174  0.29785007]])]\n",
      "A_t        = [tensor([[-0.4484,  1.0540,  0.0305, -0.4769,  1.5635],\n",
      "        [-1.0746,  0.9731, -1.0777, -1.1209, -0.4109],\n",
      "       ....5013],\n",
      "        [ 0.7467,  0.2934,  0.6407,  0.2455, -0.8840],\n",
      "        [ 2.7281,  3.3226, -0.2855,  0.3290,  0.2979]])]\n",
      "_A         = [array([[-0.4484034 ,  1.0540462 ,  0.03045906, -0.4768823 ,  1.5634812 ],\n",
      "       [-1.0746397 ,  0.9731278 , -1.077711...4550702, -0.8840323 ],\n",
      "       [ 2.7280824 ,  3.322596  , -0.28550744,  0.32897174,  0.29785007]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.4484034   1.0540462   0.03045906 -0.4768823   1.5634812 ]\n",
      " [-1.0746397   0.9731278  -1.0777118  -1...3593  0.29344648  0.64067996  0.24550702 -0.8840323 ]\n",
      " [ 2.7280824   3.322596   -0.28550744  0.32897174  0.29785007]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.4484034   1.0540462   0.03045906 -0.4768823   1.5634812 ]\n",
      " [-1.0746397   0.9731...93  0.29344648  0.64067996  0.24550702 -0.8840323 ]\n",
      " [ 2.7280824   3.322596   -0.28550744  0.32897174  0.29785007]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812fad90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.4484034   1.0540462   0.03045906 -0.4768823   1.5634812 ]\n",
      " [-1.0746397   0.9731...93  0.29344648  0.64067996  0.24550702 -0.8840323 ]\n",
      " [ 2.7280824   3.322596   -0.28550744  0.32897174  0.29785007]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7812fad90>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812faf40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812faf40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-0.4484034   1.0540462   0.03045906 -0.4768823   1.5634812 ]\n",
      " [-1.0746397   0.9731278  -1.0777118  -1.12089...8  0.64067996  0.24550702 -0.8840323 ]\n",
      " [ 2.7280824   3.322596   -0.28550744  0.32897174  0.29785007]], device=cuda()))\n",
      "        array_num  = 2\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812fad90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cuda()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812fafd0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      ">       out = ndl.stack(A, axis=axis)\n",
      "\n",
      "A          = [needle.Tensor([[[ 0.38333625  0.18051435  0.1305337   0.8501626   0.6712771\n",
      "    1.1948769  -0.30687174]\n",
      "  [-0.4117157...   -0.30100098  0.6343639 ]\n",
      "  [-0.04002846 -0.7133349   1.1940389   0.39805946 -1.54246\n",
      "   -1.0406394  -1.0654556 ]]])]\n",
      "A_t        = [tensor([[[ 0.3833,  0.1805,  0.1305,  0.8502,  0.6713,  1.1949, -0.3069],\n",
      "         [-0.4117, -0.2897,  0.5881,  1.263...1644,  0.5059,  0.1374, -0.3010,  0.6344],\n",
      "         [-0.0400, -0.7133,  1.1940,  0.3981, -1.5425, -1.0406, -1.0655]]])]\n",
      "_A         = [array([[[ 0.38333625,  0.18051435,  0.1305337 ,  0.8501626 ,\n",
      "          0.6712771 ,  1.1948769 , -0.30687174],\n",
      "       ...[-0.04002846, -0.7133349 ,  1.1940389 ,  0.39805946,\n",
      "         -1.54246   , -1.0406394 , -1.0654556 ]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:154: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[ 0.38333625  0.18051435  0.1305337   0.8501626   0.6712771\n",
      "    1.1948769  -0.30687174]\n",
      "  [-0.4117157...   -0.30100098  0.6343639 ]\n",
      "  [-0.04002846 -0.7133349   1.1940389   0.39805946 -1.54246\n",
      "   -1.0406394  -1.0654556 ]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[ 0.38333625  0.18051435  0.1305337   0.8501626   0.6712771\n",
      "    1.1948769  -0.3068... -0.30100098  0.6343639 ]\n",
      "  [-0.04002846 -0.7133349   1.1940389   0.39805946 -1.54246\n",
      "   -1.0406394  -1.0654556 ]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc781386220>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[ 0.38333625  0.18051435  0.1305337   0.8501626   0.6712771\n",
      "    1.1948769  -0.3068... -0.30100098  0.6343639 ]\n",
      "  [-0.04002846 -0.7133349   1.1940389   0.39805946 -1.54246\n",
      "   -1.0406394  -1.0654556 ]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc781386220>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781386250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781386250>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[[ 0.38333625  0.18051435  0.1305337   0.8501626   0.6712771\n",
      "    1.1948769  -0.30687174]\n",
      "  [-0.41171578 -0.2... 0.6343639 ]\n",
      "  [-0.04002846 -0.7133349   1.1940389   0.39805946 -1.54246\n",
      "   -1.0406394  -1.0654556 ]]], device=cuda()))\n",
      "        array_num  = 5\n",
      "        array_shape = [1, 5, 7]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc781386220>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cuda()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812de040>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape0-0-1] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n",
      " [-0.19517688  1.2884692   1.625865   -1...0993  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n",
      " [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]])]\n",
      "A_t        = [tensor([[-0.2649, -2.1368, -0.2813,  0.5420, -0.8280],\n",
      "        [-0.1952,  1.2885,  1.6259, -1.3193,  0.6103],\n",
      "       ...3294,  0.6092,  1.7408, -0.1585,  0.4973],\n",
      "        [ 0.7943,  0.8823, -1.2204, -0.5734, -0.6012]], requires_grad=True)]\n",
      "_A         = [array([[-0.26491052, -2.136809  , -0.28127787,  0.54197794, -0.8279596 ],\n",
      "       [-0.19517688,  1.2884692 ,  1.625865...5845154,  0.4973323 ],\n",
      "       [ 0.7943122 ,  0.8822589 , -1.2203852 , -0.5734197 , -0.6012362 ]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n",
      " [-0.19517688  1.2884692   1.625865   -1...0993  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n",
      " [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n",
      " [-0.19517688  1.2884...3  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n",
      " [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812ee880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n",
      " [-0.19517688  1.2884...3  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n",
      " [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7812ee880>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812ee940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812ee940>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n",
      " [-0.19517688  1.2884692   1.625865   -1.31933...7  1.7407578  -0.15845154  0.4973323 ]\n",
      " [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]], device=cpu()),)\n",
      "        array_num  = 1\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812ee880>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cpu()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cpu()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cpu(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812eea90>\n",
      "device     = cpu()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape1-0-2] ______________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n",
      " [-0.73380625 -1.1747231  -0.3217507   0...209   0.0315434   0.55054754 -0.9497181  -0.03779113]\n",
      " [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]])]\n",
      "A_t        = [tensor([[-0.2230,  0.0861,  0.1369,  1.5095, -0.7127],\n",
      "        [-0.7338, -1.1747, -0.3218,  0.8628, -1.1587],\n",
      "       ...7436,  0.0315,  0.5505, -0.9497, -0.0378],\n",
      "        [-1.6837, -1.3373,  1.0488,  2.2584,  0.9400]], requires_grad=True)]\n",
      "_A         = [array([[-0.2229816 ,  0.08610313,  0.13694113,  1.5095038 , -0.7126971 ],\n",
      "       [-0.73380625, -1.1747231 , -0.321750...497181 , -0.03779113],\n",
      "       [-1.6837281 , -1.3372594 ,  1.0487753 ,  2.258363  ,  0.94000864]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cpu()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n",
      " [-0.73380625 -1.1747231  -0.3217507   0...209   0.0315434   0.55054754 -0.9497181  -0.03779113]\n",
      " [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n",
      " [-0.73380625 -1.1747...9   0.0315434   0.55054754 -0.9497181  -0.03779113]\n",
      " [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7813d9f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n",
      " [-0.73380625 -1.1747...9   0.0315434   0.55054754 -0.9497181  -0.03779113]\n",
      " [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7813d9f10>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7813d9070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7813d9070>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n",
      " [-0.73380625 -1.1747231  -0.3217507   0.86278...4   0.55054754 -0.9497181  -0.03779113]\n",
      " [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]], device=cpu()))\n",
      "        array_num  = 2\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7813d9f10>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cpu()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cpu()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cpu(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7813d9190>\n",
      "device     = cpu()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape2-2-5] ______________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n",
      "   -1.0202792  -0.12881304]\n",
      "  [-0.1319975... 0.8690877   0.2656894 ]\n",
      "  [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n",
      "   -1.0931357   0.24931994]]])]\n",
      "A_t        = [tensor([[[ 1.4049,  2.0662,  1.9755,  0.7066, -1.1856, -1.0203, -0.1288],\n",
      "         [-0.1320, -0.5643,  0.2627, -0.298...8691,  0.2657],\n",
      "         [ 1.7977,  0.2424, -0.9547, -0.7499,  0.0561, -1.0931,  0.2493]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[ 1.4048624 ,  2.0662467 ,  1.975455  ,  0.70656043,\n",
      "         -1.1856185 , -1.0202792 , -0.12881304],\n",
      "       ...[ 1.7976619 ,  0.24236043, -0.95474315, -0.74990946,\n",
      "          0.05611507, -1.0931357 ,  0.24931994]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cpu()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n",
      "   -1.0202792  -0.12881304]\n",
      "  [-0.1319975... 0.8690877   0.2656894 ]\n",
      "  [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n",
      "   -1.0931357   0.24931994]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n",
      "   -1.0202792  -0.1288....8690877   0.2656894 ]\n",
      "  [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n",
      "   -1.0931357   0.24931994]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812c1a30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n",
      "   -1.0202792  -0.1288....8690877   0.2656894 ]\n",
      "  [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n",
      "   -1.0931357   0.24931994]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7812c1a30>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812c1f10>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7812c1f10>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n",
      "   -1.0202792  -0.12881304]\n",
      "  [-0.13199753 -0.5....2656894 ]\n",
      "  [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n",
      "   -1.0931357   0.24931994]]], device=cpu()))\n",
      "        array_num  = 5\n",
      "        array_shape = [1, 5, 7]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7812c1a30>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cpu()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cpu()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cpu(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7812c1eb0>\n",
      "device     = cpu()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-1.122568    1.3219295  -0.14445436 -0.9865747   0.2616311 ]\n",
      " [ 0.93468    -0.37505797  0.25460657  0...7374  0.82490486  0.5144381  -0.14003062 -0.20621192]\n",
      " [-0.23054962 -2.1563048  -1.2538761   1.090643   -0.51554435]])]\n",
      "A_t        = [tensor([[-1.1226,  1.3219, -0.1445, -0.9866,  0.2616],\n",
      "        [ 0.9347, -0.3751,  0.2546,  0.2845,  0.7044],\n",
      "       ...3590,  0.8249,  0.5144, -0.1400, -0.2062],\n",
      "        [-0.2305, -2.1563, -1.2539,  1.0906, -0.5155]], requires_grad=True)]\n",
      "_A         = [array([[-1.122568  ,  1.3219295 , -0.14445436, -0.9865747 ,  0.2616311 ],\n",
      "       [ 0.93468   , -0.37505797,  0.254606...4003062, -0.20621192],\n",
      "       [-0.23054962, -2.1563048 , -1.2538761 ,  1.090643  , -0.51554435]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 0\n",
      "l          = 1\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-1.122568    1.3219295  -0.14445436 -0.9865747   0.2616311 ]\n",
      " [ 0.93468    -0.37505797  0.25460657  0...7374  0.82490486  0.5144381  -0.14003062 -0.20621192]\n",
      " [-0.23054962 -2.1563048  -1.2538761   1.090643   -0.51554435]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.122568    1.3219295  -0.14445436 -0.9865747   0.2616311 ]\n",
      " [ 0.93468    -0.3750...4  0.82490486  0.5144381  -0.14003062 -0.20621192]\n",
      " [-0.23054962 -2.1563048  -1.2538761   1.090643   -0.51554435]]),),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc781393af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.122568    1.3219295  -0.14445436 -0.9865747   0.2616311 ]\n",
      " [ 0.93468    -0.3750...4  0.82490486  0.5144381  -0.14003062 -0.20621192]\n",
      " [-0.23054962 -2.1563048  -1.2538761   1.090643   -0.51554435]]),),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc781393af0>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781393ca0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781393ca0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-1.122568    1.3219295  -0.14445436 -0.9865747   0.2616311 ]\n",
      " [ 0.93468    -0.37505797  0.25460657  0.28449...  0.5144381  -0.14003062 -0.20621192]\n",
      " [-0.23054962 -2.1563048  -1.2538761   1.090643   -0.51554435]], device=cuda()),)\n",
      "        array_num  = 1\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc781393af0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cuda()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc781393d90>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[-1.0567564  -0.30265552  0.4281822  -0.68831074  1.6516227 ]\n",
      " [ 0.74296576 -0.27638575  0.76607263 -0...424   0.68060917  0.5635482   1.0085729   0.75971615]\n",
      " [ 0.31350297 -2.0439312  -0.09097444  0.11011965 -0.23238643]])]\n",
      "A_t        = [tensor([[-1.0568, -0.3027,  0.4282, -0.6883,  1.6516],\n",
      "        [ 0.7430, -0.2764,  0.7661, -0.8939,  1.2894],\n",
      "       ...6141,  0.6806,  0.5635,  1.0086,  0.7597],\n",
      "        [ 0.3135, -2.0439, -0.0910,  0.1101, -0.2324]], requires_grad=True)]\n",
      "_A         = [array([[-1.0567564 , -0.30265552,  0.4281822 , -0.68831074,  1.6516227 ],\n",
      "       [ 0.74296576, -0.27638575,  0.766072...085729 ,  0.75971615],\n",
      "       [ 0.31350297, -2.0439312 , -0.09097444,  0.11011965, -0.23238643]],\n",
      "      dtype=float32)]\n",
      "axis       = 0\n",
      "device     = cuda()\n",
      "i          = 1\n",
      "l          = 2\n",
      "shape      = (5, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[-1.0567564  -0.30265552  0.4281822  -0.68831074  1.6516227 ]\n",
      " [ 0.74296576 -0.27638575  0.76607263 -0...424   0.68060917  0.5635482   1.0085729   0.75971615]\n",
      " [ 0.31350297 -2.0439312  -0.09097444  0.11011965 -0.23238643]])]\n",
      "        axis       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[-1.0567564  -0.30265552  0.4281822  -0.68831074  1.6516227 ]\n",
      " [ 0.74296576 -0.2763...4   0.68060917  0.5635482   1.0085729   0.75971615]\n",
      " [ 0.31350297 -2.0439312  -0.09097444  0.11011965 -0.23238643]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc7813ae3a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[-1.0567564  -0.30265552  0.4281822  -0.68831074  1.6516227 ]\n",
      " [ 0.74296576 -0.2763...4   0.68060917  0.5635482   1.0085729   0.75971615]\n",
      " [ 0.31350297 -2.0439312  -0.09097444  0.11011965 -0.23238643]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc7813ae3a0>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7813ae4f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc7813ae4f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[-1.0567564  -0.30265552  0.4281822  -0.68831074  1.6516227 ]\n",
      " [ 0.74296576 -0.27638575  0.76607263 -0.89387...7  0.5635482   1.0085729   0.75971615]\n",
      " [ 0.31350297 -2.0439312  -0.09097444  0.11011965 -0.23238643]], device=cuda()))\n",
      "        array_num  = 2\n",
      "        array_shape = [5, 5]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc7813ae3a0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cuda()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc7813ae5e0>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n",
      "\n",
      "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\n",
      "        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\n",
      "            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\n",
      ">       ndl.stack(A, axis=axis).sum().backward()\n",
      "\n",
      "A          = [needle.Tensor([[[-1.2567368  -0.00447594  0.03316377  0.4079615  -0.13135351\n",
      "   -1.1835566   0.15766405]\n",
      "  [ 0.406209... -1.9483194   0.21689712]\n",
      "  [ 0.43637753 -0.49504337  1.4166641  -0.2632424   1.5031598\n",
      "   -1.2881242  -0.24279231]]])]\n",
      "A_t        = [tensor([[[-1.2567, -0.0045,  0.0332,  0.4080, -0.1314, -1.1836,  0.1577],\n",
      "         [ 0.4062, -0.9746, -0.8885, -0.661...9483,  0.2169],\n",
      "         [ 0.4364, -0.4950,  1.4167, -0.2632,  1.5032, -1.2881, -0.2428]]],\n",
      "       requires_grad=True)]\n",
      "_A         = [array([[[-1.2567368 , -0.00447594,  0.03316377,  0.4079615 ,\n",
      "         -0.13135351, -1.1835566 ,  0.15766405],\n",
      "       ...[ 0.43637753, -0.49504337,  1.4166641 , -0.2632424 ,\n",
      "          1.5031598 , -1.2881242 , -0.24279231]]], dtype=float32)]\n",
      "axis       = 2\n",
      "device     = cuda()\n",
      "i          = 4\n",
      "l          = 5\n",
      "shape      = (1, 5, 7)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:167: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:500: in stack\n",
      "    \u001b[94mreturn\u001b[39;49;00m Stack(axis)(make_tuple(*args))\n",
      "        args       = [needle.Tensor([[[-1.2567368  -0.00447594  0.03316377  0.4079615  -0.13135351\n",
      "   -1.1835566   0.15766405]\n",
      "  [ 0.406209... -1.9483194   0.21689712]\n",
      "  [ 0.43637753 -0.49504337  1.4166641  -0.2632424   1.5031598\n",
      "   -1.2881242  -0.24279231]]])]\n",
      "        axis       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.TensorTuple(needle.Tensor([[[-1.2567368  -0.00447594  0.03316377  0.4079615  -0.13135351\n",
      "   -1.1835566   0.157...1.9483194   0.21689712]\n",
      "  [ 0.43637753 -0.49504337  1.4166641  -0.2632424   1.5031598\n",
      "   -1.2881242  -0.24279231]]])),)\n",
      "        self       = <needle.ops.Stack object at 0x7fc781324370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.TensorTuple(needle.Tensor([[[-1.2567368  -0.00447594  0.03316377  0.4079615  -0.13135351\n",
      "   -1.1835566   0.157...1.9483194   0.21689712]\n",
      "  [ 0.43637753 -0.49504337  1.4166641  -0.2632424   1.5031598\n",
      "   -1.2881242  -0.24279231]]])),)\n",
      "        op         = <needle.ops.Stack object at 0x7fc781324370>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781324940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not iterable\") raised in repr()] Tensor object at 0x7fc781324940>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:474: in compute\n",
      "    new_array = array_api.empty(\n",
      "        args       = (NDArray([[[-1.2567368  -0.00447594  0.03316377  0.4079615  -0.13135351\n",
      "   -1.1835566   0.15766405]\n",
      "  [ 0.40620932 -0.....21689712]\n",
      "  [ 0.43637753 -0.49504337  1.4166641  -0.2632424   1.5031598\n",
      "   -1.2881242  -0.24279231]]], device=cuda()))\n",
      "        array_num  = 5\n",
      "        array_shape = [1, 5, 7]\n",
      "        new_shape  = None\n",
      "        self       = <needle.ops.Stack object at 0x7fc781324370>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:636: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m device.empty(shape, dtype)\n",
      "        device     = cuda()\n",
      "        dtype      = 'float32'\n",
      "        shape      = None\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:48: in empty\n",
      "    \u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\n",
      "        dtype      = 'float32'\n",
      "        self       = cuda()\n",
      "        shape      = None\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = None, strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[37m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\n",
      ">       array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not iterable\u001b[0m\n",
      "\n",
      "array      = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7fc781324a00>\n",
      "device     = cuda()\n",
      "handle     = None\n",
      "offset     = 0\n",
      "shape      = None\n",
      "strides    = None\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:140: TypeError\n",
      "\u001b[31m\u001b[1m___________________ test_summation_backward[cpu-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[0.00565341]]])\n",
      "_A         = array([[[0.00565341]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[0.00565341]]]),)\n",
      "        c          = array([0.68353326])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = 0.0038711267480868675\n",
      "        f2         = 0.0038574559788166676\n",
      "        i          = 0\n",
      "        j          = 0\n",
      "        kwargs     = {'axes': None}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[0.68353846]]])]\n",
      "        out        = needle.Tensor([0.00565341])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([0.00565341])\n",
      "        out_grad   = needle.Tensor([0.68353325])\n",
      "        self       = <needle.ops.Summation object at 0x7fc78116f0d0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc78116f0d0>\n",
      "out_grad = needle.Tensor([0.68353325]), node = needle.Tensor([0.00565341])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      "            \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "                new_shape[a] = \u001b[94m1\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      ">           new_shape = array_api.ones(\u001b[96mlen\u001b[39;49;00m(in_shape), dtype=\u001b[96mint\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'ones'\u001b[0m\n",
      "\n",
      "in_shape   = (1, 1, 1)\n",
      "node       = needle.Tensor([0.00565341])\n",
      "out_grad   = needle.Tensor([0.68353325])\n",
      "self       = <needle.ops.Summation object at 0x7fc78116f0d0>\n",
      "x          = needle.Tensor([[[0.00565341]]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:295: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape1-0] _____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[-1.1112776  -2.5426693   0.51752055]\n",
      " [ 0.47009262  0.3695671  -0.42903185]\n",
      " [-0.4613905   0.24671845  1.2265509 ]\n",
      " [ 0.43084052  0.54698414  0.00562425]\n",
      " [-0.09525896  0.66889954 -1.8471652 ]])\n",
      "_A         = array([[-1.1112776 , -2.5426693 ,  0.51752055],\n",
      "       [ 0.4700926 ,  0.36956707, -0.42903188],\n",
      "       [-0.46139053,  ...5509 ],\n",
      "       [ 0.4308405 ,  0.54698414,  0.00562425],\n",
      "       [-0.09525896,  0.66889954, -1.8471652 ]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[-1.1112776  -2.5426693   0.51752055]\n",
      " [ 0.47009262  0.3695671  -0.42903185]\n",
      " [-0.4613905   0.24671845  1.2265509 ]\n",
      " [ 0.43084052  0.54698414  0.00562425]\n",
      " [-0.09525896  0.66889954 -1.8471652 ]]),)\n",
      "        c          = array([ 0.97829075, -0.30162265,  0.88071494])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = -0.9997288714623455\n",
      "        f2         = -0.9997465096819906\n",
      "        i          = 0\n",
      "        j          = 14\n",
      "        kwargs     = {'axes': 0}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[ 0.9796193 , -0.30203226,  0.88191098],\n",
      "       [ 0.97378824, -0.30203226,  0.88191098],\n",
      "       [ 0.9796193 , ...203226,  0.88191098],\n",
      "       [ 0.9796193 , -0.30203226,  0.88191098],\n",
      "       [ 0.9796193 , -0.30203226,  0.88191098]])]\n",
      "        out        = needle.Tensor([-0.766994  -0.7105    -0.5265013])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([-0.766994  -0.7105    -0.5265013])\n",
      "        out_grad   = needle.Tensor([ 0.97829074 -0.30162266  0.88071495])\n",
      "        self       = <needle.ops.Summation object at 0x7fc781440730>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc781440730>\n",
      "out_grad = needle.Tensor([ 0.97829074 -0.30162266  0.88071495])\n",
      "node = needle.Tensor([-0.766994  -0.7105    -0.5265013])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "in_shape   = (5, 3)\n",
      "new_shape  = [5, 3]\n",
      "node       = needle.Tensor([-0.766994  -0.7105    -0.5265013])\n",
      "out_grad   = needle.Tensor([ 0.97829074 -0.30162266  0.88071495])\n",
      "self       = <needle.ops.Summation object at 0x7fc781440730>\n",
      "x          = needle.Tensor([[-1.1112776  -2.5426693   0.51752055]\n",
      " [ 0.47009262  0.3695671  -0.42903185]\n",
      " [-0.4613905   0.24671845  1.2265509 ]\n",
      " [ 0.43084052  0.54698414  0.00562425]\n",
      " [-0.09525896  0.66889954 -1.8471652 ]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:292: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape2-1] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[ 1.2571155   1.7787892 ]\n",
      "  [ 0.97692966 -0.472893  ]\n",
      "  [ 1.3856629   1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0....3]\n",
      "  [ 1.175274   -0.02697281]]\n",
      "\n",
      " [[ 0.05634464 -0.22966217]\n",
      "  [-1.2457664   0.1854648 ]\n",
      "  [-0.26671386 -0.31800154]]])\n",
      "_A         = array([[[ 1.2571155 ,  1.7787892 ],\n",
      "        [ 0.97692966, -0.47289303],\n",
      "        [ 1.3856629 ,  1.1309735 ]],\n",
      "\n",
      "       [...  [[ 0.05634464, -0.22966217],\n",
      "        [-1.2457664 ,  0.1854648 ],\n",
      "        [-0.2667139 , -0.31800157]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[ 1.2571155   1.7787892 ]\n",
      "  [ 0.97692966 -0.472893  ]\n",
      "  [ 1.3856629   1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0...\n",
      "  [ 1.175274   -0.02697281]]\n",
      "\n",
      " [[ 0.05634464 -0.22966217]\n",
      "  [-1.2457664   0.1854648 ]\n",
      "  [-0.26671386 -0.31800154]]]),)\n",
      "        c          = array([[-1.28237754,  1.4503455 ],\n",
      "       [-1.73917763, -0.92885193],\n",
      "       [ 0.97068872, -1.59182185],\n",
      "       [ 0.53...-0.79526456],\n",
      "       [-0.11231625,  1.36421335],\n",
      "       [ 0.05703132, -0.44432947],\n",
      "       [-0.76478755,  1.42819903]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = -2.245690729734225\n",
      "        f2         = -2.245719247378238\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 1}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-1.28411905,  1.45231511],\n",
      "        [-1.28411905,  1.45231511],\n",
      "        [-1.28411905,  1.45231511]],\n",
      "\n",
      "       ...3288]],\n",
      "\n",
      "       [[-0.76582616,  1.42801038],\n",
      "        [-0.76582616,  1.42801038],\n",
      "        [-0.76582616,  1.4258822 ]]])]\n",
      "        out        = needle.Tensor([[ 3.619708    2.4368696 ]\n",
      " [ 0.5683019   4.0216413 ]\n",
      " [ 1.2932744   0.06329054]\n",
      " [-1.4859312   2.005837  ]\n",
      " [ 0.9063595   0.75122166]\n",
      " [-0.6476835  -0.33710006]\n",
      " [-0.08712578 -1.1888998 ]\n",
      " [-1.4561356  -0.36219895]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[ 3.619708    2.4368696 ]\n",
      " [ 0.5683019   4.0216413 ]\n",
      " [ 1.2932744   0.06329054]\n",
      " [-1.4859312   2.005837  ]\n",
      " [ 0.9063595   0.75122166]\n",
      " [-0.6476835  -0.33710006]\n",
      " [-0.08712578 -1.1888998 ]\n",
      " [-1.4561356  -0.36219895]])\n",
      "        out_grad   = needle.Tensor([[-1.2823775   1.4503455 ]\n",
      " [-1.7391776  -0.9288519 ]\n",
      " [ 0.9706887  -1.5918218 ]\n",
      " [ 0.53199524  1.245671  ]\n",
      " [ 0.64864075 -0.79526454]\n",
      " [-0.11231624  1.3642133 ]\n",
      " [ 0.05703132 -0.44432947]\n",
      " [-0.76478755  1.428199  ]])\n",
      "        self       = <needle.ops.Summation object at 0x7fc7813d38b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc7813d38b0>\n",
      "out_grad = needle.Tensor([[-1.2823775   1.4503455 ]\n",
      " [-1.7391776  -0.9288519 ]\n",
      " [ 0.9706887  -1.5918218 ]\n",
      " [ 0.53199524  1.245671  ]\n",
      " [ 0.64864075 -0.79526454]\n",
      " [-0.11231624  1.3642133 ]\n",
      " [ 0.05703132 -0.44432947]\n",
      " [-0.76478755  1.428199  ]])\n",
      "node = needle.Tensor([[ 3.619708    2.4368696 ]\n",
      " [ 0.5683019   4.0216413 ]\n",
      " [ 1.2932744   0.06329054]\n",
      " [-1.4859312   2.005837  ]\n",
      " [ 0.9063595   0.75122166]\n",
      " [-0.6476835  -0.33710006]\n",
      " [-0.08712578 -1.1888998 ]\n",
      " [-1.4561356  -0.36219895]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "in_shape   = (8, 3, 2)\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[ 3.619708    2.4368696 ]\n",
      " [ 0.5683019   4.0216413 ]\n",
      " [ 1.2932744   0.06329054]\n",
      " [-1.4859312   2.005837  ]\n",
      " [ 0.9063595   0.75122166]\n",
      " [-0.6476835  -0.33710006]\n",
      " [-0.08712578 -1.1888998 ]\n",
      " [-1.4561356  -0.36219895]])\n",
      "out_grad   = needle.Tensor([[-1.2823775   1.4503455 ]\n",
      " [-1.7391776  -0.9288519 ]\n",
      " [ 0.9706887  -1.5918218 ]\n",
      " [ 0.53199524  1.245671  ]\n",
      " [ 0.64864075 -0.79526454]\n",
      " [-0.11231624  1.3642133 ]\n",
      " [ 0.05703132 -0.44432947]\n",
      " [-0.76478755  1.428199  ]])\n",
      "self       = <needle.ops.Summation object at 0x7fc7813d38b0>\n",
      "x          = needle.Tensor([[[ 1.2571155   1.7787892 ]\n",
      "  [ 0.97692966 -0.472893  ]\n",
      "  [ 1.3856629   1.1309735 ]]\n",
      "\n",
      " [[-0.23845685  0....3]\n",
      "  [ 1.175274   -0.02697281]]\n",
      "\n",
      " [[ 0.05634464 -0.22966217]\n",
      "  [-1.2457664   0.1854648 ]\n",
      "  [-0.26671386 -0.31800154]]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:292: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cpu-shape3-2] _____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-0.42389694  0.8514999 ]\n",
      "  [ 1.2843655   0.41561052]\n",
      "  [ 2.2930398  -0.02262507]]\n",
      "\n",
      " [[-1.1791399  -0.... ]\n",
      "  [ 0.58287376  0.7526748 ]]\n",
      "\n",
      " [[-0.4980525   0.2263798 ]\n",
      "  [-0.5305156  -0.39114535]\n",
      "  [ 0.24308126  1.0304061 ]]])\n",
      "_A         = array([[[-0.42389697,  0.8514999 ],\n",
      "        [ 1.2843655 ,  0.4156105 ],\n",
      "        [ 2.2930398 , -0.02262507]],\n",
      "\n",
      "       [...  [[-0.49805254,  0.2263798 ],\n",
      "        [-0.5305156 , -0.39114538],\n",
      "        [ 0.24308126,  1.0304061 ]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[-0.42389694  0.8514999 ]\n",
      "  [ 1.2843655   0.41561052]\n",
      "  [ 2.2930398  -0.02262507]]\n",
      "\n",
      " [[-1.1791399  -0...\n",
      "  [ 0.58287376  0.7526748 ]]\n",
      "\n",
      " [[-0.4980525   0.2263798 ]\n",
      "  [-0.5305156  -0.39114535]\n",
      "  [ 0.24308126  1.0304061 ]]]),)\n",
      "        c          = array([[-1.42742278, -0.92912381, -0.36237938],\n",
      "       [ 1.36608848,  0.35763026,  1.58604805],\n",
      "       [-0.30664139, -...7672454,  1.66926069],\n",
      "       [ 0.35276007, -0.66973841, -1.18520704],\n",
      "       [-1.17584978,  1.190126  , -0.14738184]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = -5.854173719036078\n",
      "        f2         = -5.854170767396213\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 2}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-1.42723424, -1.42936126],\n",
      "        [-0.93038559, -0.92484758],\n",
      "        [-0.36287151, -0.36287151]],\n",
      "\n",
      "       ...1659]],\n",
      "\n",
      "       [[-1.17744662, -1.17569447],\n",
      "        [ 1.19174223,  1.18819537],\n",
      "        [-0.14758199, -0.14758199]]])]\n",
      "        out        = needle.Tensor([[ 0.42760295  1.699976    2.2704148 ]\n",
      " [-1.2662436  -1.4708     -1.2472318 ]\n",
      " [-2.3992321  -1.8330529  ...\n",
      " [-0.78681195 -0.7035869  -2.1041868 ]\n",
      " [ 0.569461    0.02130485  1.3355486 ]\n",
      " [-0.27167273 -0.921661    1.2734873 ]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[ 0.42760295  1.699976    2.2704148 ]\n",
      " [-1.2662436  -1.4708     -1.2472318 ]\n",
      " [-2.3992321  -1.8330529  ...\n",
      " [-0.78681195 -0.7035869  -2.1041868 ]\n",
      " [ 0.569461    0.02130485  1.3355486 ]\n",
      " [-0.27167273 -0.921661    1.2734873 ]])\n",
      "        out_grad   = needle.Tensor([[-1.4274228  -0.9291238  -0.36237937]\n",
      " [ 1.3660885   0.35763025  1.586048  ]\n",
      " [-0.3066414  -1.4592648  ...\n",
      " [ 0.5975249  -0.37672454  1.6692607 ]\n",
      " [ 0.35276008 -0.6697384  -1.185207  ]\n",
      " [-1.1758498   1.190126   -0.14738184]])\n",
      "        self       = <needle.ops.Summation object at 0x7fc7811d4ca0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc7811d4ca0>\n",
      "out_grad = needle.Tensor([[-1.4274228  -0.9291238  -0.36237937]\n",
      " [ 1.3660885   0.35763025  1.586048  ]\n",
      " [-0.3066414  -1.4592648  ...\n",
      " [ 0.5975249  -0.37672454  1.6692607 ]\n",
      " [ 0.35276008 -0.6697384  -1.185207  ]\n",
      " [-1.1758498   1.190126   -0.14738184]])\n",
      "node = needle.Tensor([[ 0.42760295  1.699976    2.2704148 ]\n",
      " [-1.2662436  -1.4708     -1.2472318 ]\n",
      " [-2.3992321  -1.8330529  ...\n",
      " [-0.78681195 -0.7035869  -2.1041868 ]\n",
      " [ 0.569461    0.02130485  1.3355486 ]\n",
      " [-0.27167273 -0.921661    1.2734873 ]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "in_shape   = (8, 3, 2)\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[ 0.42760295  1.699976    2.2704148 ]\n",
      " [-1.2662436  -1.4708     -1.2472318 ]\n",
      " [-2.3992321  -1.8330529  ...\n",
      " [-0.78681195 -0.7035869  -2.1041868 ]\n",
      " [ 0.569461    0.02130485  1.3355486 ]\n",
      " [-0.27167273 -0.921661    1.2734873 ]])\n",
      "out_grad   = needle.Tensor([[-1.4274228  -0.9291238  -0.36237937]\n",
      " [ 1.3660885   0.35763025  1.586048  ]\n",
      " [-0.3066414  -1.4592648  ...\n",
      " [ 0.5975249  -0.37672454  1.6692607 ]\n",
      " [ 0.35276008 -0.6697384  -1.185207  ]\n",
      " [-1.1758498   1.190126   -0.14738184]])\n",
      "self       = <needle.ops.Summation object at 0x7fc7811d4ca0>\n",
      "x          = needle.Tensor([[[-0.42389694  0.8514999 ]\n",
      "  [ 1.2843655   0.41561052]\n",
      "  [ 2.2930398  -0.02262507]]\n",
      "\n",
      " [[-1.1791399  -0.... ]\n",
      "  [ 0.58287376  0.7526748 ]]\n",
      "\n",
      " [[-0.4980525   0.2263798 ]\n",
      "  [-0.5305156  -0.39114535]\n",
      "  [ 0.24308126  1.0304061 ]]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:292: TypeError\n",
      "\u001b[31m\u001b[1m__________________ test_summation_backward[cuda-shape0-None] ___________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-0.24800439]]])\n",
      "_A         = array([[[-0.24800439]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[-0.24800439]]]),)\n",
      "        c          = array([-1.54952729])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = 0.3842740802200008\n",
      "        f2         = 0.3843050666724754\n",
      "        i          = 0\n",
      "        j          = 0\n",
      "        kwargs     = {'axes': None}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[-1.54932262]]])]\n",
      "        out        = needle.Tensor([-0.24800439])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([-0.24800439])\n",
      "        out_grad   = needle.Tensor([-1.5495273])\n",
      "        self       = <needle.ops.Summation object at 0x7fc781348bb0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc781348bb0>\n",
      "out_grad = needle.Tensor([-1.5495273]), node = needle.Tensor([-0.24800439])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      "            \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "                new_shape[a] = \u001b[94m1\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      ">           new_shape = array_api.ones(\u001b[96mlen\u001b[39;49;00m(in_shape), dtype=\u001b[96mint\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE           AttributeError: module 'needle.backend_ndarray' has no attribute 'ones'\u001b[0m\n",
      "\n",
      "in_shape   = (1, 1, 1)\n",
      "node       = needle.Tensor([-0.24800439])\n",
      "out_grad   = needle.Tensor([-1.5495273])\n",
      "self       = <needle.ops.Summation object at 0x7fc781348bb0>\n",
      "x          = needle.Tensor([[[-0.24800439]]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:295: AttributeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[-0.1203487  -1.5250406  -0.10352791]\n",
      " [ 1.8576714  -0.23217203  1.2052398 ]\n",
      " [ 0.8887145   0.7708414  -0.61790377]\n",
      " [ 0.14626923  0.7474238  -0.01467989]\n",
      " [ 0.7175262  -1.1380908   0.4594319 ]])\n",
      "_A         = array([[-0.1203487 , -1.5250406 , -0.10352791],\n",
      "       [ 1.8576714 , -0.23217203,  1.2052398 ],\n",
      "       [ 0.8887145 ,  ...90377],\n",
      "       [ 0.14626923,  0.7474238 , -0.01467989],\n",
      "       [ 0.7175262 , -1.1380908 ,  0.45943186]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[-0.1203487  -1.5250406  -0.10352791]\n",
      " [ 1.8576714  -0.23217203  1.2052398 ]\n",
      " [ 0.8887145   0.7708414  -0.61790377]\n",
      " [ 0.14626923  0.7474238  -0.01467989]\n",
      " [ 0.7175262  -1.1380908   0.4594319 ]]),)\n",
      "        c          = array([ 0.86492562, -0.5685719 , -0.13818243])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = 3.6730788607572094\n",
      "        f2         = 3.6730816199225402\n",
      "        i          = 0\n",
      "        j          = 14\n",
      "        kwargs     = {'axes': 0}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[ 0.86610022, -0.56934404, -0.13837008],\n",
      "       [ 0.86610022, -0.56934404, -0.13837008],\n",
      "       [ 0.86610022, ...934404, -0.13837008],\n",
      "       [ 0.86610022, -0.56934404, -0.13795827],\n",
      "       [ 0.86610022, -0.56934404, -0.13795827]])]\n",
      "        out        = needle.Tensor([ 3.4898326 -1.3770382  0.9285601])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([ 3.4898326 -1.3770382  0.9285601])\n",
      "        out_grad   = needle.Tensor([ 0.8649256  -0.5685719  -0.13818243])\n",
      "        self       = <needle.ops.Summation object at 0x7fc7813cfb50>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc7813cfb50>\n",
      "out_grad = needle.Tensor([ 0.8649256  -0.5685719  -0.13818243])\n",
      "node = needle.Tensor([ 3.4898326 -1.3770382  0.9285601])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "in_shape   = (5, 3)\n",
      "new_shape  = [5, 3]\n",
      "node       = needle.Tensor([ 3.4898326 -1.3770382  0.9285601])\n",
      "out_grad   = needle.Tensor([ 0.8649256  -0.5685719  -0.13818243])\n",
      "self       = <needle.ops.Summation object at 0x7fc7813cfb50>\n",
      "x          = needle.Tensor([[-0.1203487  -1.5250406  -0.10352791]\n",
      " [ 1.8576714  -0.23217203  1.2052398 ]\n",
      " [ 0.8887145   0.7708414  -0.61790377]\n",
      " [ 0.14626923  0.7474238  -0.01467989]\n",
      " [ 0.7175262  -1.1380908   0.4594319 ]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:292: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.02258228  0.7159469 ]\n",
      "  [ 1.4280046   0.04938466]\n",
      "  [ 0.277153    0.9073988 ]]\n",
      "\n",
      " [[-1.7968049  -0.... ]\n",
      "  [-0.16331597  1.5508778 ]]\n",
      "\n",
      " [[-0.7730272   0.2436368 ]\n",
      "  [ 0.6098334   0.57733613]\n",
      "  [ 0.24750309  1.7347882 ]]])\n",
      "_A         = array([[[ 0.02258228,  0.7159469 ],\n",
      "        [ 1.4280046 ,  0.04938466],\n",
      "        [ 0.27715296,  0.9073988 ]],\n",
      "\n",
      "       [...  [[-0.7730272 ,  0.2436368 ],\n",
      "        [ 0.6098334 ,  0.57733613],\n",
      "        [ 0.24750309,  1.7347882 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[ 0.02258228  0.7159469 ]\n",
      "  [ 1.4280046   0.04938466]\n",
      "  [ 0.277153    0.9073988 ]]\n",
      "\n",
      " [[-1.7968049  -0...\n",
      "  [-0.16331597  1.5508778 ]]\n",
      "\n",
      " [[-0.7730272   0.2436368 ]\n",
      "  [ 0.6098334   0.57733613]\n",
      "  [ 0.24750309  1.7347882 ]]]),)\n",
      "        c          = array([[ 1.97808523, -2.10939137],\n",
      "       [ 0.0146967 ,  0.07289008],\n",
      "       [ 0.33634784,  1.12971303],\n",
      "       [ 0.01...-0.76121954],\n",
      "       [ 0.73192888,  0.65431253],\n",
      "       [ 1.69228165, -0.85450624],\n",
      "       [-0.35046999, -0.64244955]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = -3.2809940051362623\n",
      "        f2         = -3.2809811386959846\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 1}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[ 1.98077154, -2.11225599],\n",
      "        [ 1.98077154, -2.11225599],\n",
      "        [ 1.96898123, -2.11225599]],\n",
      "\n",
      "       ...6668]],\n",
      "\n",
      "       [[-0.35094594, -0.64332201],\n",
      "        [-0.35094594, -0.64332201],\n",
      "        [-0.3504237 , -0.64332201]]])]\n",
      "        out        = needle.Tensor([[ 1.7277398e+00  1.6727304e+00]\n",
      " [-6.5985620e-01  1.0175824e-02]\n",
      " [ 7.2903144e-01 -2.4748440e+00]\n",
      " [ 4....04  1.5632931e+00]\n",
      " [ 9.0821218e-01  2.4341617e+00]\n",
      " [ 9.8911211e-02  5.1713729e-01]\n",
      " [ 8.4309325e-02  2.5557611e+00]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[ 1.7277398e+00  1.6727304e+00]\n",
      " [-6.5985620e-01  1.0175824e-02]\n",
      " [ 7.2903144e-01 -2.4748440e+00]\n",
      " [ 4....04  1.5632931e+00]\n",
      " [ 9.0821218e-01  2.4341617e+00]\n",
      " [ 9.8911211e-02  5.1713729e-01]\n",
      " [ 8.4309325e-02  2.5557611e+00]])\n",
      "        out_grad   = needle.Tensor([[ 1.9780853  -2.1093915 ]\n",
      " [ 0.0146967   0.07289008]\n",
      " [ 0.33634785  1.129713  ]\n",
      " [ 0.01987992 -0.21903647]\n",
      " [ 0.09909772 -0.76121956]\n",
      " [ 0.7319289   0.65431255]\n",
      " [ 1.6922816  -0.85450625]\n",
      " [-0.35047    -0.64244956]])\n",
      "        self       = <needle.ops.Summation object at 0x7fc781378e20>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc781378e20>\n",
      "out_grad = needle.Tensor([[ 1.9780853  -2.1093915 ]\n",
      " [ 0.0146967   0.07289008]\n",
      " [ 0.33634785  1.129713  ]\n",
      " [ 0.01987992 -0.21903647]\n",
      " [ 0.09909772 -0.76121956]\n",
      " [ 0.7319289   0.65431255]\n",
      " [ 1.6922816  -0.85450625]\n",
      " [-0.35047    -0.64244956]])\n",
      "node = needle.Tensor([[ 1.7277398e+00  1.6727304e+00]\n",
      " [-6.5985620e-01  1.0175824e-02]\n",
      " [ 7.2903144e-01 -2.4748440e+00]\n",
      " [ 4....04  1.5632931e+00]\n",
      " [ 9.0821218e-01  2.4341617e+00]\n",
      " [ 9.8911211e-02  5.1713729e-01]\n",
      " [ 8.4309325e-02  2.5557611e+00]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "in_shape   = (8, 3, 2)\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[ 1.7277398e+00  1.6727304e+00]\n",
      " [-6.5985620e-01  1.0175824e-02]\n",
      " [ 7.2903144e-01 -2.4748440e+00]\n",
      " [ 4....04  1.5632931e+00]\n",
      " [ 9.0821218e-01  2.4341617e+00]\n",
      " [ 9.8911211e-02  5.1713729e-01]\n",
      " [ 8.4309325e-02  2.5557611e+00]])\n",
      "out_grad   = needle.Tensor([[ 1.9780853  -2.1093915 ]\n",
      " [ 0.0146967   0.07289008]\n",
      " [ 0.33634785  1.129713  ]\n",
      " [ 0.01987992 -0.21903647]\n",
      " [ 0.09909772 -0.76121956]\n",
      " [ 0.7319289   0.65431255]\n",
      " [ 1.6922816  -0.85450625]\n",
      " [-0.35047    -0.64244956]])\n",
      "self       = <needle.ops.Summation object at 0x7fc781378e20>\n",
      "x          = needle.Tensor([[[ 0.02258228  0.7159469 ]\n",
      "  [ 1.4280046   0.04938466]\n",
      "  [ 0.277153    0.9073988 ]]\n",
      "\n",
      " [[-1.7968049  -0.... ]\n",
      "  [-0.16331597  1.5508778 ]]\n",
      "\n",
      " [[-0.7730272   0.2436368 ]\n",
      "  [ 0.6098334   0.57733613]\n",
      "  [ 0.24750309  1.7347882 ]]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:292: TypeError\n",
      "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      ">       backward_check(ndl.summation, A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[-0.31651038 -0.55685735]\n",
      "  [ 0.17516482 -0.59294105]\n",
      "  [ 0.16579469 -1.4521552 ]]\n",
      "\n",
      " [[-1.1660721  -1....9]\n",
      "  [ 1.0682702   0.87369984]]\n",
      "\n",
      " [[-1.4355507   0.1892495 ]\n",
      "  [-1.1416229  -0.20496817]\n",
      "  [-0.13278429 -0.4402109 ]]])\n",
      "_A         = array([[[-0.3165104 , -0.55685735],\n",
      "        [ 0.17516482, -0.59294105],\n",
      "        [ 0.16579469, -1.4521552 ]],\n",
      "\n",
      "       [...  [[-1.4355507 ,  0.1892495 ],\n",
      "        [-1.1416229 , -0.20496817],\n",
      "        [-0.13278429, -0.44021094]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:191: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:28: in backward_check\n",
      "    backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\n",
      "        args       = (needle.Tensor([[[-0.31651038 -0.55685735]\n",
      "  [ 0.17516482 -0.59294105]\n",
      "  [ 0.16579469 -1.4521552 ]]\n",
      "\n",
      " [[-1.1660721  -1...\n",
      "  [ 1.0682702   0.87369984]]\n",
      "\n",
      " [[-1.4355507   0.1892495 ]\n",
      "  [-1.1416229  -0.20496817]\n",
      "  [-0.13278429 -0.4402109 ]]]),)\n",
      "        c          = array([[ 0.8286478 ,  0.04491751,  1.97075877],\n",
      "       [-0.63587435, -1.30128528, -0.74705432],\n",
      "       [ 1.29118656,  ...5579318,  0.79195079],\n",
      "       [-0.46331693,  0.78626659, -1.07966071],\n",
      "       [ 0.55414091,  1.40031508,  0.38172382]])\n",
      "        eps        = 1e-05\n",
      "        f          = <function summation at 0x7fc80842d1f0>\n",
      "        f1         = -4.9316758921025965\n",
      "        f2         = -4.9316835141942725\n",
      "        i          = 0\n",
      "        j          = 47\n",
      "        kwargs     = {'axes': 2}\n",
      "        num_args   = 1\n",
      "        numerical_grad = [array([[[ 0.82730356,  0.82977313],\n",
      "        [ 0.04497851,  0.04497851],\n",
      "        [ 1.97343512,  1.97343512]],\n",
      "\n",
      "       ...2692]],\n",
      "\n",
      "       [[ 0.55489345,  0.55489345],\n",
      "        [ 1.40221675,  1.39387023],\n",
      "        [ 0.38110458,  0.38110458]]])]\n",
      "        out        = needle.Tensor([[-0.8733678  -0.41777623 -1.2863605 ]\n",
      " [-2.7558427  -0.15774459  0.675676  ]\n",
      " [ 1.7372644  -0.79743975 ...\n",
      " [ 2.7325518  -0.1369676  -0.08027595]\n",
      " [-1.1898752  -2.0540318   1.9419701 ]\n",
      " [-1.2463012  -1.3465911  -0.57299525]])\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:60: in gradient_as_tuple\n",
      "    output = \u001b[96mself\u001b[39;49;00m.gradient(out_grad, node)\n",
      "        node       = needle.Tensor([[-0.8733678  -0.41777623 -1.2863605 ]\n",
      " [-2.7558427  -0.15774459  0.675676  ]\n",
      " [ 1.7372644  -0.79743975 ...\n",
      " [ 2.7325518  -0.1369676  -0.08027595]\n",
      " [-1.1898752  -2.0540318   1.9419701 ]\n",
      " [-1.2463012  -1.3465911  -0.57299525]])\n",
      "        out_grad   = needle.Tensor([[ 0.8286478   0.04491751  1.9707588 ]\n",
      " [-0.63587433 -1.3012853  -0.74705434]\n",
      " [ 1.2911866   0.6756286  ...\n",
      " [ 0.74715984 -1.9557931   0.79195076]\n",
      " [-0.46331692  0.78626657 -1.0796607 ]\n",
      " [ 0.5541409   1.400315    0.38172382]])\n",
      "        self       = <needle.ops.Summation object at 0x7fc7813a29a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Summation object at 0x7fc7813a29a0>\n",
      "out_grad = needle.Tensor([[ 0.8286478   0.04491751  1.9707588 ]\n",
      " [-0.63587433 -1.3012853  -0.74705434]\n",
      " [ 1.2911866   0.6756286  ...\n",
      " [ 0.74715984 -1.9557931   0.79195076]\n",
      " [-0.46331692  0.78626657 -1.0796607 ]\n",
      " [ 0.5541409   1.400315    0.38172382]])\n",
      "node = needle.Tensor([[-0.8733678  -0.41777623 -1.2863605 ]\n",
      " [-2.7558427  -0.15774459  0.675676  ]\n",
      " [ 1.7372644  -0.79743975 ...\n",
      " [ 2.7325518  -0.1369676  -0.08027595]\n",
      " [-1.1898752  -2.0540318   1.9419701 ]\n",
      " [-1.2463012  -1.3465911  -0.57299525]])\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mgradient\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, out_grad, node):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        x = node.inputs[\u001b[94m0\u001b[39;49;00m]\n",
      "        in_shape = x.numpy().shape\n",
      "    \n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            new_shape = \u001b[96mlist\u001b[39;49;00m(in_shape)\n",
      ">           \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "\u001b[1m\u001b[31mE           TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n",
      "in_shape   = (8, 3, 2)\n",
      "new_shape  = [8, 3, 2]\n",
      "node       = needle.Tensor([[-0.8733678  -0.41777623 -1.2863605 ]\n",
      " [-2.7558427  -0.15774459  0.675676  ]\n",
      " [ 1.7372644  -0.79743975 ...\n",
      " [ 2.7325518  -0.1369676  -0.08027595]\n",
      " [-1.1898752  -2.0540318   1.9419701 ]\n",
      " [-1.2463012  -1.3465911  -0.57299525]])\n",
      "out_grad   = needle.Tensor([[ 0.8286478   0.04491751  1.9707588 ]\n",
      " [-0.63587433 -1.3012853  -0.74705434]\n",
      " [ 1.2911866   0.6756286  ...\n",
      " [ 0.74715984 -1.9557931   0.79195076]\n",
      " [-0.46331692  0.78626657 -1.0796607 ]\n",
      " [ 0.5541409   1.400315    0.38172382]])\n",
      "self       = <needle.ops.Summation object at 0x7fc7813a29a0>\n",
      "x          = needle.Tensor([[[-0.31651038 -0.55685735]\n",
      "  [ 0.17516482 -0.59294105]\n",
      "  [ 0.16579469 -1.4521552 ]]\n",
      "\n",
      " [[-1.1660721  -1....9]\n",
      "  [ 1.0682702   0.87369984]]\n",
      "\n",
      " [[-1.4355507   0.1892495 ]\n",
      "  [-1.1416229  -0.20496817]\n",
      "  [-0.13278429 -0.4402109 ]]])\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:292: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape0] ________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.69669217]]])\n",
      "_A         = array([[[-0.69669217]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:218: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.69669217]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.69669217]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc7814578b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.69669217]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc7814578b0>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc781457fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc781457fd0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc7814578b0>\n",
      "a = NDArray([[[-0.69669217]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        axes_list = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(a.ndim))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            axes_list[-\u001b[94m2\u001b[39;49;00m:-\u001b[94m1\u001b[39;49;00m:] = axes_list[-\u001b[94m1\u001b[39;49;00m:-\u001b[94m2\u001b[39;49;00m:]\n",
      "        axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = (\n",
      ">           \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m],\n",
      "            \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m],\n",
      "        )\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not subscriptable\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.69669217]]], device=cpu())\n",
      "axes_list  = [0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc7814578b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:198: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cpu-None-shape1] ________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311....9787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]])\n",
      "_A         = array([[[-0.93575615, -0.61421335, -0.45289811,  0.96114016,\n",
      "          0.23405291, -0.8781786 ],\n",
      "        [-0.40377602,...342],\n",
      "        [ 1.1850599 , -0.7730635 ,  1.5469892 , -0.71304744,\n",
      "          0.92298496, -0.52088904]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cpu()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:218: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311....9787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.361931...787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc7813a2820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.361931...787657   0.12266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc7813a2820>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc7813a2700>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc7813a2700>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc7813a2820>\n",
      "a = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...2266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cpu())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        axes_list = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(a.ndim))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            axes_list[-\u001b[94m2\u001b[39;49;00m:-\u001b[94m1\u001b[39;49;00m:] = axes_list[-\u001b[94m1\u001b[39;49;00m:-\u001b[94m2\u001b[39;49;00m:]\n",
      "        axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = (\n",
      ">           \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m],\n",
      "            \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m],\n",
      "        )\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not subscriptable\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.93575615 -0.61421335 -0.45289811  0.96114016  0.23405291\n",
      "   -0.8781786 ]\n",
      "  [-0.40377602  0.3619311  -0.0...2266027\n",
      "    0.46679342]\n",
      "  [ 1.1850599  -0.7730635   1.5469892  -0.71304744  0.92298496\n",
      "   -0.52088904]]], device=cpu())\n",
      "axes_list  = [0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc7813a2820>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:198: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.4497797]]])\n",
      "_A         = array([[[-0.4497797]]], dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (1, 1, 1)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:218: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-0.4497797]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.4497797]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc781302730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.4497797]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc781302730>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc7813022b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc7813022b0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc781302730>\n",
      "a = NDArray([[[-0.4497797]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        axes_list = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(a.ndim))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            axes_list[-\u001b[94m2\u001b[39;49;00m:-\u001b[94m1\u001b[39;49;00m:] = axes_list[-\u001b[94m1\u001b[39;49;00m:-\u001b[94m2\u001b[39;49;00m:]\n",
      "        axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = (\n",
      ">           \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m],\n",
      "            \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m],\n",
      "        )\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not subscriptable\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-0.4497797]]], device=cuda())\n",
      "axes_list  = [0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc781302730>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:198: TypeError\n",
      "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), axes = None, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            np_axes = axes\n",
      ">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-4.32244986e-01  6.26861870e-01 -1.80563629e+00  6.71942651e-01\n",
      "   -8.02810416e-02  1.66190493e+00]\n",
      " ...66648531e-01]\n",
      "  [-1.49845636e+00  1.16017960e-01 -4.71417069e-01  3.20715189e-01\n",
      "   -7.65091628e-02 -6.89490438e-01]]])\n",
      "_A         = array([[[-4.32244986e-01,  6.26861870e-01, -1.80563629e+00,\n",
      "          6.71942651e-01, -8.02810416e-02,  1.66190493e+00...,  1.16017960e-01, -4.71417069e-01,\n",
      "          3.20715189e-01, -7.65091628e-02, -6.89490438e-01]]],\n",
      "      dtype=float32)\n",
      "axes       = None\n",
      "device     = cuda()\n",
      "np_axes    = (1, 2)\n",
      "shape      = (4, 5, 6)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:226: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:218: in transpose\n",
      "    \u001b[94mreturn\u001b[39;49;00m Transpose(axes)(a)\n",
      "        a          = needle.Tensor([[[-4.32244986e-01  6.26861870e-01 -1.80563629e+00  6.71942651e-01\n",
      "   -8.02810416e-02  1.66190493e+00]\n",
      " ...66648531e-01]\n",
      "  [-1.49845636e+00  1.16017960e-01 -4.71417069e-01  3.20715189e-01\n",
      "   -7.65091628e-02 -6.89490438e-01]]])\n",
      "        axes       = None\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-4.32244986e-01  6.26861870e-01 -1.80563629e+00  6.71942651e-01\n",
      "   -8.02810416e-02  1.66190493e+00]\n",
      "...648531e-01]\n",
      "  [-1.49845636e+00  1.16017960e-01 -4.71417069e-01  3.20715189e-01\n",
      "   -7.65091628e-02 -6.89490438e-01]]]),)\n",
      "        self       = <needle.ops.Transpose object at 0x7fc78148f820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-4.32244986e-01  6.26861870e-01 -1.80563629e+00  6.71942651e-01\n",
      "   -8.02810416e-02  1.66190493e+00]\n",
      "...648531e-01]\n",
      "  [-1.49845636e+00  1.16017960e-01 -4.71417069e-01  3.20715189e-01\n",
      "   -7.65091628e-02 -6.89490438e-01]]]),)\n",
      "        op         = <needle.ops.Transpose object at 0x7fc78148f820>\n",
      "        tensor     = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc78148f6a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError(\"'NoneType' object is not subscriptable\") raised in repr()] Tensor object at 0x7fc78148f6a0>\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <needle.ops.Transpose object at 0x7fc78148f820>\n",
      "a = NDArray([[[-4.32244986e-01  6.26861870e-01 -1.80563629e+00  6.71942651e-01\n",
      "   -8.02810416e-02  1.66190493e+00]\n",
      "  [-9.0... [-1.49845636e+00  1.16017960e-01 -4.71417069e-01  3.20715189e-01\n",
      "   -7.65091628e-02 -6.89490438e-01]]], device=cuda())\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\n",
      "        axes_list = \u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(a.ndim))\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.axes:\n",
      "            axes_list[-\u001b[94m2\u001b[39;49;00m:-\u001b[94m1\u001b[39;49;00m:] = axes_list[-\u001b[94m1\u001b[39;49;00m:-\u001b[94m2\u001b[39;49;00m:]\n",
      "        axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m]], axes_list[\u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m]] = (\n",
      ">           \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m1\u001b[39;49;00m],\n",
      "            \u001b[96mself\u001b[39;49;00m.axes[\u001b[94m0\u001b[39;49;00m],\n",
      "        )\n",
      "\u001b[1m\u001b[31mE       TypeError: 'NoneType' object is not subscriptable\u001b[0m\n",
      "\n",
      "a          = NDArray([[[-4.32244986e-01  6.26861870e-01 -1.80563629e+00  6.71942651e-01\n",
      "   -8.02810416e-02  1.66190493e+00]\n",
      "  [-9.0... [-1.49845636e+00  1.16017960e-01 -4.71417069e-01  3.20715189e-01\n",
      "   -7.65091628e-02 -6.89490438e-01]]], device=cuda())\n",
      "axes_list  = [0, 2]\n",
      "self       = <needle.ops.Transpose object at 0x7fc78148f820>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:198: TypeError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]])\n",
      "A_t        = tensor([[-1.4687,  1.5395,  0.4671],\n",
      "        [-2.1181,  0.0950, -0.2642],\n",
      "        [-0.3252,  0.7073, -0.6719],\n",
      "        [-1.8797,  0.6002,  0.4975],\n",
      "        [-0.5367, -0.5332, -0.1854]])\n",
      "_A         = array([[-1.4686818 ,  1.5394609 ,  0.46706542],\n",
      "       [-2.1180692 ,  0.095046  , -0.2642157 ],\n",
      "       [-0.32524756,  ...9159 ],\n",
      "       [-1.8796829 ,  0.6002251 ,  0.4974558 ],\n",
      "       [-0.5366705 , -0.53319454, -0.18542138]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cpu()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7813f7760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc7813f7760>\n",
      "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc7813f7c40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc7813f7c40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:407: in compute\n",
      "    array_api.exp(Z - Z_max_broadcast).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        Z          = NDArray([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]], device=cpu())\n",
      "        Z_max      = NDArray([-0.32524756  1.5394609   0.4974558 ], device=cpu())\n",
      "        Z_max_broadcast = NDArray([[-0.32524756  1.5394609   0.4974558 ]], device=cpu())\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7813f7760>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:450: in __sub__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\n",
      "        other      = NDArray([[-0.32524756  1.5394609   0.4974558 ]], device=cpu())\n",
      "        self       = NDArray([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:443: in __add__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\n",
      "        other      = NDArray([[ 0.32524756 -1.5394609  -0.4974558 ]], device=cpu())\n",
      "        self       = NDArray([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]], device=cpu())\n",
      "other = NDArray([[ 0.32524756 -1.5394609  -0.4974558 ]], device=cpu())\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc8084234e0>\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808423510>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other, ewise_func, scalar_func):\n",
      "        \u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
      "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
      "\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc8084234e0>\n",
      "other      = NDArray([[ 0.32524756 -1.5394609  -0.4974558 ]], device=cpu())\n",
      "out        = NDArray([[2.6773023e-34 0.0000000e+00 2.7654092e-34]\n",
      " [0.0000000e+00 3.5873241e-43 0.0000000e+00]\n",
      " [1.1210388e-43 0.00....7932721e-34]\n",
      " [0.0000000e+00 2.7468107e-34 0.0000000e+00]\n",
      " [1.0051212e+00 2.1406875e+00 1.4788681e+00]], device=cpu())\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808423510>\n",
      "self       = NDArray([[-1.4686818   1.5394609   0.46706542]\n",
      " [-2.1180692   0.095046   -0.2642157 ]\n",
      " [-0.32524756  0.70726275 -0.6719159 ]\n",
      " [-1.8796829   0.6002251   0.4974558 ]\n",
      " [-0.5366705  -0.53319454 -0.18542138]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:436: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.... ]\n",
      "  [ 0.5479147  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]])\n",
      "A_t        = tensor([[[-0.6457, -0.1899],\n",
      "         [ 0.0643,  1.6374],\n",
      "         [-0.7709, -0.8806]],\n",
      "\n",
      "        [[-2.1338, -0.1627],\n",
      "...         [ 0.5479, -1.2947]],\n",
      "\n",
      "        [[-0.3398, -1.4804],\n",
      "         [ 0.2238,  0.6902],\n",
      "         [ 0.1464,  0.6198]]])\n",
      "_A         = array([[[-0.64573014, -0.18989763],\n",
      "        [ 0.06425801,  1.6373779 ],\n",
      "        [-0.77092874, -0.8805883 ]],\n",
      "\n",
      "       [...  [[-0.33982575, -1.4803531 ],\n",
      "        [ 0.22382198,  0.6902278 ],\n",
      "        [ 0.146406  ,  0.6197508 ]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.... ]\n",
      "  [ 0.5479147  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0...\n",
      "  [ 0.5479147  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc781344e20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0...\n",
      "  [ 0.5479147  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc781344e20>\n",
      "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc781344190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc781344190>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:407: in compute\n",
      "    array_api.exp(Z - Z_max_broadcast).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        Z          = NDArray([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.162731...47  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]], device=cpu())\n",
      "        Z_max      = NDArray([[0.06425801 1.6373779 ]\n",
      " [0.02922101 1.546231  ]\n",
      " [1.1865426  0.6630871 ]\n",
      " [0.5586132  2.3734741 ]\n",
      " [2.3574812  1.2975326 ]\n",
      " [0.242455   1.5860875 ]\n",
      " [1.1591896  1.13256   ]\n",
      " [0.22382198 0.6902278 ]], device=cpu())\n",
      "        Z_max_broadcast = NDArray([[[0.06425801 1.6373779 ]]\n",
      "\n",
      " [[0.02922101 1.546231  ]]\n",
      "\n",
      " [[1.1865426  0.6630871 ]]\n",
      "\n",
      " [[0.5586132  2.3734741 ]]...74812  1.2975326 ]]\n",
      "\n",
      " [[0.242455   1.5860875 ]]\n",
      "\n",
      " [[1.1591896  1.13256   ]]\n",
      "\n",
      " [[0.22382198 0.6902278 ]]], device=cpu())\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc781344e20>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:450: in __sub__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\n",
      "        other      = NDArray([[[0.06425801 1.6373779 ]]\n",
      "\n",
      " [[0.02922101 1.546231  ]]\n",
      "\n",
      " [[1.1865426  0.6630871 ]]\n",
      "\n",
      " [[0.5586132  2.3734741 ]]...74812  1.2975326 ]]\n",
      "\n",
      " [[0.242455   1.5860875 ]]\n",
      "\n",
      " [[1.1591896  1.13256   ]]\n",
      "\n",
      " [[0.22382198 0.6902278 ]]], device=cpu())\n",
      "        self       = NDArray([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.162731...47  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:443: in __add__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\n",
      "        other      = NDArray([[[-0.06425801 -1.6373779 ]]\n",
      "\n",
      " [[-0.02922101 -1.546231  ]]\n",
      "\n",
      " [[-1.1865426  -0.6630871 ]]\n",
      "\n",
      " [[-0.5586132  -2.37...-1.2975326 ]]\n",
      "\n",
      " [[-0.242455   -1.5860875 ]]\n",
      "\n",
      " [[-1.1591896  -1.13256   ]]\n",
      "\n",
      " [[-0.22382198 -0.6902278 ]]], device=cpu())\n",
      "        self       = NDArray([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.162731...47  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.162731...47  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]], device=cpu())\n",
      "other = NDArray([[[-0.06425801 -1.6373779 ]]\n",
      "\n",
      " [[-0.02922101 -1.546231  ]]\n",
      "\n",
      " [[-1.1865426  -0.6630871 ]]\n",
      "\n",
      " [[-0.5586132  -2.37...-1.2975326 ]]\n",
      "\n",
      " [[-0.242455   -1.5860875 ]]\n",
      "\n",
      " [[-1.1591896  -1.13256   ]]\n",
      "\n",
      " [[-0.22382198 -0.6902278 ]]], device=cpu())\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc8084234e0>\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808423510>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other, ewise_func, scalar_func):\n",
      "        \u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
      "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
      "\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc8084234e0>\n",
      "other      = NDArray([[[-0.06425801 -1.6373779 ]]\n",
      "\n",
      " [[-0.02922101 -1.546231  ]]\n",
      "\n",
      " [[-1.1865426  -0.6630871 ]]\n",
      "\n",
      " [[-0.5586132  -2.37...-1.2975326 ]]\n",
      "\n",
      " [[-0.242455   -1.5860875 ]]\n",
      "\n",
      " [[-1.1591896  -1.13256   ]]\n",
      "\n",
      " [[-0.22382198 -0.6902278 ]]], device=cpu())\n",
      "out        = NDArray([[[-0.64573014  0.06425801]\n",
      "  [-0.77092874 -0.18989763]\n",
      "  [ 1.6373779  -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -1.681554...    -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575  0.22382198]\n",
      "  [ 0.146406   -1.4803531 ]\n",
      "  [ 0.6902278   0.6197508 ]]], device=cpu())\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808423510>\n",
      "self       = NDArray([[[-0.64573014 -0.18989763]\n",
      "  [ 0.06425801  1.6373779 ]\n",
      "  [-0.77092874 -0.8805883 ]]\n",
      "\n",
      " [[-2.1338394  -0.162731...47  -1.2947313 ]]\n",
      "\n",
      " [[-0.33982575 -1.4803531 ]\n",
      "  [ 0.22382198  0.6902278 ]\n",
      "  [ 0.146406    0.6197508 ]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:436: AssertionError\n",
      "\u001b[31m\u001b[1m_________________________ test_logsumexp[cpu-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.... ]\n",
      "  [ 0.27359074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]])\n",
      "A_t        = tensor([[[-0.4174,  1.1506],\n",
      "         [ 1.3620, -1.1809],\n",
      "         [-1.1867,  0.7175]],\n",
      "\n",
      "        [[-0.6472, -0.2910],\n",
      "...         [ 0.2736,  0.8715]],\n",
      "\n",
      "        [[ 2.0847, -0.7713],\n",
      "         [ 1.0185,  1.6994],\n",
      "         [ 0.5326,  0.0303]]])\n",
      "_A         = array([[[-0.41739646,  1.1505585 ],\n",
      "        [ 1.3620341 , -1.1808528 ],\n",
      "        [-1.1866927 ,  0.7174831 ]],\n",
      "\n",
      "       [...  [[ 2.0846891 , -0.77134025],\n",
      "        [ 1.0184602 ,  1.699401  ],\n",
      "        [ 0.53261983,  0.03025856]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cpu()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.... ]\n",
      "  [ 0.27359074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0...\n",
      "  [ 0.27359074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7813932b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0...\n",
      "  [ 0.27359074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc7813932b0>\n",
      "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc781393d60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc781393d60>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:407: in compute\n",
      "    array_api.exp(Z - Z_max_broadcast).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        Z          = NDArray([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.291011...074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]], device=cpu())\n",
      "        Z_max      = NDArray([[ 1.1505585   1.3620341   0.7174831 ]\n",
      " [-0.29101115  1.2864434   2.442721  ]\n",
      " [-0.75733256  0.6846819  -0.706...  0.19765173  0.34433335]\n",
      " [ 1.6630172   2.8018577   0.8715183 ]\n",
      " [ 2.0846891   1.699401    0.53261983]], device=cpu())\n",
      "        Z_max_broadcast = NDArray([[[ 1.1505585 ]\n",
      "  [ 1.3620341 ]\n",
      "  [ 0.7174831 ]]\n",
      "\n",
      " [[-0.29101115]\n",
      "  [ 1.2864434 ]\n",
      "  [ 2.442721  ]]\n",
      "\n",
      " [[-0.7573...5]]\n",
      "\n",
      " [[ 1.6630172 ]\n",
      "  [ 2.8018577 ]\n",
      "  [ 0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891 ]\n",
      "  [ 1.699401  ]\n",
      "  [ 0.53261983]]], device=cpu())\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7813932b0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:450: in __sub__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\n",
      "        other      = NDArray([[[ 1.1505585 ]\n",
      "  [ 1.3620341 ]\n",
      "  [ 0.7174831 ]]\n",
      "\n",
      " [[-0.29101115]\n",
      "  [ 1.2864434 ]\n",
      "  [ 2.442721  ]]\n",
      "\n",
      " [[-0.7573...5]]\n",
      "\n",
      " [[ 1.6630172 ]\n",
      "  [ 2.8018577 ]\n",
      "  [ 0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891 ]\n",
      "  [ 1.699401  ]\n",
      "  [ 0.53261983]]], device=cpu())\n",
      "        self       = NDArray([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.291011...074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]], device=cpu())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:443: in __add__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\n",
      "        other      = NDArray([[[-1.1505585 ]\n",
      "  [-1.3620341 ]\n",
      "  [-0.7174831 ]]\n",
      "\n",
      " [[ 0.29101115]\n",
      "  [-1.2864434 ]\n",
      "  [-2.442721  ]]\n",
      "\n",
      " [[ 0.7573...5]]\n",
      "\n",
      " [[-1.6630172 ]\n",
      "  [-2.8018577 ]\n",
      "  [-0.8715183 ]]\n",
      "\n",
      " [[-2.0846891 ]\n",
      "  [-1.699401  ]\n",
      "  [-0.53261983]]], device=cpu())\n",
      "        self       = NDArray([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.291011...074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.291011...074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]], device=cpu())\n",
      "other = NDArray([[[-1.1505585 ]\n",
      "  [-1.3620341 ]\n",
      "  [-0.7174831 ]]\n",
      "\n",
      " [[ 0.29101115]\n",
      "  [-1.2864434 ]\n",
      "  [-2.442721  ]]\n",
      "\n",
      " [[ 0.7573...5]]\n",
      "\n",
      " [[-1.6630172 ]\n",
      "  [-2.8018577 ]\n",
      "  [-0.8715183 ]]\n",
      "\n",
      " [[-2.0846891 ]\n",
      "  [-1.699401  ]\n",
      "  [-0.53261983]]], device=cpu())\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc8084234e0>\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808423510>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other, ewise_func, scalar_func):\n",
      "        \u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
      "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
      "\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc8084234e0>\n",
      "other      = NDArray([[[-1.1505585 ]\n",
      "  [-1.3620341 ]\n",
      "  [-0.7174831 ]]\n",
      "\n",
      " [[ 0.29101115]\n",
      "  [-1.2864434 ]\n",
      "  [-2.442721  ]]\n",
      "\n",
      " [[ 0.7573...5]]\n",
      "\n",
      " [[-1.6630172 ]\n",
      "  [-2.8018577 ]\n",
      "  [-0.8715183 ]]\n",
      "\n",
      " [[-2.0846891 ]\n",
      "  [-1.699401  ]\n",
      "  [-0.53261983]]], device=cpu())\n",
      "out        = NDArray([[[ 2.6323420e-08  4.5839275e-41]\n",
      "  [ 2.6323420e-08  4.5839275e-41]\n",
      "  [-1.1866927e+00  7.1748310e-01]]\n",
      "\n",
      " [[-6....\n",
      " [[ 2.0846891e+00 -7.7134025e-01]\n",
      "  [ 1.0184602e+00  1.6994010e+00]\n",
      "  [ 5.3261983e-01  3.0258557e-02]]], device=cpu())\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808423510>\n",
      "self       = NDArray([[[-0.41739646  1.1505585 ]\n",
      "  [ 1.3620341  -1.1808528 ]\n",
      "  [-1.1866927   0.7174831 ]]\n",
      "\n",
      " [[-0.64722466 -0.291011...074  0.8715183 ]]\n",
      "\n",
      " [[ 2.0846891  -0.77134025]\n",
      "  [ 1.0184602   1.699401  ]\n",
      "  [ 0.53261983  0.03025856]]], device=cpu())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:436: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n",
      "\n",
      "shape = (5, 3), axes = 0, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.848...1e+00 -2.7298880e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]])\n",
      "A_t        = tensor([[ 2.1940e-01,  2.1156e-01,  1.1877e+00],\n",
      "        [ 8.6819e-01,  8.1104e-04,  2.2587e-01],\n",
      "        [ 6.8488e-01...5e+00, -2.7299e+00],\n",
      "        [-8.9583e-01, -7.6962e-01,  1.6294e-01],\n",
      "        [ 8.8078e-01, -3.2714e-01, -8.3769e-01]])\n",
      "_A         = array([[ 2.1940093e-01,  2.1155696e-01,  1.1876984e+00],\n",
      "       [ 8.6819392e-01,  8.1103557e-04,  2.2586522e-01],\n",
      "    ....9583379e-01, -7.6962298e-01,  1.6293789e-01],\n",
      "       [ 8.8077790e-01, -3.2713804e-01, -8.3769453e-01]], dtype=float32)\n",
      "axes       = 0\n",
      "device     = cuda()\n",
      "shape      = (5, 3)\n",
      "t_axes     = 0\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.848...1e+00 -2.7298880e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]])\n",
      "        axes       = 0\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.84...+00 -2.7298880e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7814515e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.84...+00 -2.7298880e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc7814515e0>\n",
      "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc7814515b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc7814515b0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:407: in compute\n",
      "    array_api.exp(Z - Z_max_broadcast).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        Z          = NDArray([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.8488491e-...0e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]], device=cuda())\n",
      "        Z_max      = NDArray([0.8807779 1.0915481 1.1876984], device=cuda())\n",
      "        Z_max_broadcast = NDArray([[0.8807779 1.0915481 1.1876984]], device=cuda())\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7814515e0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:450: in __sub__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\n",
      "        other      = NDArray([[0.8807779 1.0915481 1.1876984]], device=cuda())\n",
      "        self       = NDArray([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.8488491e-...0e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:443: in __add__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\n",
      "        other      = NDArray([[-0.8807779 -1.0915481 -1.1876984]], device=cuda())\n",
      "        self       = NDArray([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.8488491e-...0e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.8488491e-...0e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]], device=cuda())\n",
      "other = NDArray([[-0.8807779 -1.0915481 -1.1876984]], device=cuda())\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc808491630>\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808491660>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other, ewise_func, scalar_func):\n",
      "        \u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
      "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
      "\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc808491630>\n",
      "other      = NDArray([[-0.8807779 -1.0915481 -1.1876984]], device=cuda())\n",
      "out        = NDArray([[-0.45871726 -0.41777623 -1.2863605 ]\n",
      " [-2.7558427  -0.15774459  0.675676  ]\n",
      " [ 1.7372644  -0.79743975  1.656289  ]\n",
      " [-1.0162696   2.8794465  -1.6030592 ]\n",
      " [-0.84057194 -1.087667   -1.754452  ]], device=cuda())\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808491660>\n",
      "self       = NDArray([[ 2.1940093e-01  2.1155696e-01  1.1876984e+00]\n",
      " [ 8.6819392e-01  8.1103557e-04  2.2586522e-01]\n",
      " [ 6.8488491e-...0e+00]\n",
      " [-8.9583379e-01 -7.6962298e-01  1.6293789e-01]\n",
      " [ 8.8077790e-01 -3.2713804e-01 -8.3769453e-01]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:436: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 1, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1....6]\n",
      "  [-0.06918286  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]])\n",
      "A_t        = tensor([[[-0.9397,  1.6033],\n",
      "         [ 1.4388, -0.8560],\n",
      "         [ 0.4211, -1.9551]],\n",
      "\n",
      "        [[ 1.8886, -1.2960],\n",
      "...         [-0.0692,  0.8882]],\n",
      "\n",
      "        [[-0.3923,  0.6581],\n",
      "         [-1.7375, -0.1404],\n",
      "         [-0.0906,  0.9108]]])\n",
      "_A         = array([[[-0.9397055 ,  1.6032794 ],\n",
      "        [ 1.4387522 , -0.85600376],\n",
      "        [ 0.42110795, -1.9550894 ]],\n",
      "\n",
      "       [...  [[-0.3923111 ,  0.65810174],\n",
      "        [-1.7375134 , -0.14040655],\n",
      "        [-0.09061285,  0.91081566]]], dtype=float32)\n",
      "axes       = 1\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 1\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1....6]\n",
      "  [-0.06918286  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]])\n",
      "        axes       = 1\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1...\n",
      "  [-0.06918286  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc78133f4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1...\n",
      "  [-0.06918286  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc78133f4c0>\n",
      "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc78133f820>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc78133f820>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:407: in compute\n",
      "    array_api.exp(Z - Z_max_broadcast).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        Z          = NDArray([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1.296007...86  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]], device=cuda())\n",
      "        Z_max      = NDArray([[ 1.4387522   1.6032794 ]\n",
      " [ 1.8886433   2.211625  ]\n",
      " [ 2.3187726   0.9561986 ]\n",
      " [ 1.4436367   0.17890424]\n",
      " [ 1.9934407   1.6350731 ]\n",
      " [ 2.189848   -0.08896094]\n",
      " [ 1.953528    0.88816154]\n",
      " [-0.09061285  0.91081566]], device=cuda())\n",
      "        Z_max_broadcast = NDArray([[[ 1.4387522   1.6032794 ]]\n",
      "\n",
      " [[ 1.8886433   2.211625  ]]\n",
      "\n",
      " [[ 2.3187726   0.9561986 ]]\n",
      "\n",
      " [[ 1.4436367   0.17...1.6350731 ]]\n",
      "\n",
      " [[ 2.189848   -0.08896094]]\n",
      "\n",
      " [[ 1.953528    0.88816154]]\n",
      "\n",
      " [[-0.09061285  0.91081566]]], device=cuda())\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc78133f4c0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:450: in __sub__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\n",
      "        other      = NDArray([[[ 1.4387522   1.6032794 ]]\n",
      "\n",
      " [[ 1.8886433   2.211625  ]]\n",
      "\n",
      " [[ 2.3187726   0.9561986 ]]\n",
      "\n",
      " [[ 1.4436367   0.17...1.6350731 ]]\n",
      "\n",
      " [[ 2.189848   -0.08896094]]\n",
      "\n",
      " [[ 1.953528    0.88816154]]\n",
      "\n",
      " [[-0.09061285  0.91081566]]], device=cuda())\n",
      "        self       = NDArray([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1.296007...86  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:443: in __add__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\n",
      "        other      = NDArray([[[-1.4387522  -1.6032794 ]]\n",
      "\n",
      " [[-1.8886433  -2.211625  ]]\n",
      "\n",
      " [[-2.3187726  -0.9561986 ]]\n",
      "\n",
      " [[-1.4436367  -0.17...1.6350731 ]]\n",
      "\n",
      " [[-2.189848    0.08896094]]\n",
      "\n",
      " [[-1.953528   -0.88816154]]\n",
      "\n",
      " [[ 0.09061285 -0.91081566]]], device=cuda())\n",
      "        self       = NDArray([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1.296007...86  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1.296007...86  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]], device=cuda())\n",
      "other = NDArray([[[-1.4387522  -1.6032794 ]]\n",
      "\n",
      " [[-1.8886433  -2.211625  ]]\n",
      "\n",
      " [[-2.3187726  -0.9561986 ]]\n",
      "\n",
      " [[-1.4436367  -0.17...1.6350731 ]]\n",
      "\n",
      " [[-2.189848    0.08896094]]\n",
      "\n",
      " [[-1.953528   -0.88816154]]\n",
      "\n",
      " [[ 0.09061285 -0.91081566]]], device=cuda())\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc808491630>\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808491660>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other, ewise_func, scalar_func):\n",
      "        \u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
      "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
      "\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc808491630>\n",
      "other      = NDArray([[[-1.4387522  -1.6032794 ]]\n",
      "\n",
      " [[-1.8886433  -2.211625  ]]\n",
      "\n",
      " [[-2.3187726  -0.9561986 ]]\n",
      "\n",
      " [[-1.4436367  -0.17...1.6350731 ]]\n",
      "\n",
      " [[-2.189848    0.08896094]]\n",
      "\n",
      " [[-1.953528   -0.88816154]]\n",
      "\n",
      " [[ 0.09061285 -0.91081566]]], device=cuda())\n",
      "out        = NDArray([[[-0.1203487  -1.5250406 ]\n",
      "  [-0.10352791  1.8576714 ]\n",
      "  [-0.23217203  1.2052398 ]]\n",
      "\n",
      " [[ 0.8887145   0.770841...    0.        ]]\n",
      "\n",
      " [[ 0.          0.        ]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]], device=cuda())\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808491660>\n",
      "self       = NDArray([[[-0.9397055   1.6032794 ]\n",
      "  [ 1.4387522  -0.85600376]\n",
      "  [ 0.42110795 -1.9550894 ]]\n",
      "\n",
      " [[ 1.8886433  -1.296007...86  0.88816154]]\n",
      "\n",
      " [[-0.3923111   0.65810174]\n",
      "  [-1.7375134  -0.14040655]\n",
      "  [-0.09061285  0.91081566]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:436: AssertionError\n",
      "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n",
      "\n",
      "shape = (8, 3, 2), axes = 2, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\n",
      "        _A = np.random.randn(*shape).astype(np.float32)\n",
      "        A = ndl.Tensor(nd.array(_A), device=device)\n",
      "        A_t = torch.Tensor(_A)\n",
      "        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\n",
      "            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            t_axes = axes\n",
      ">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\n",
      "\n",
      "A          = needle.Tensor([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1....8]\n",
      "  [ 1.1119847   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]])\n",
      "A_t        = tensor([[[ 0.4265,  0.5579],\n",
      "         [ 0.3293,  0.4249],\n",
      "         [-0.4909, -0.1292]],\n",
      "\n",
      "        [[ 0.3461, -1.4637],\n",
      "...         [ 1.1120,  0.9884]],\n",
      "\n",
      "        [[-0.0534,  0.0805],\n",
      "         [ 0.5399,  0.2518],\n",
      "         [-0.7687,  0.1242]]])\n",
      "_A         = array([[[ 0.42649978,  0.55792385],\n",
      "        [ 0.32932186,  0.4249462 ],\n",
      "        [-0.49092507, -0.12920508]],\n",
      "\n",
      "       [...  [[-0.05336624,  0.08052945],\n",
      "        [ 0.53987354,  0.25176743],\n",
      "        [-0.7686626 ,  0.12419878]]], dtype=float32)\n",
      "axes       = 2\n",
      "device     = cuda()\n",
      "shape      = (8, 3, 2)\n",
      "t_axes     = 2\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:239: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:438: in logsumexp\n",
      "    \u001b[94mreturn\u001b[39;49;00m LogSumExp(axes=axes)(a)\n",
      "        a          = needle.Tensor([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1....8]\n",
      "  [ 1.1119847   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]])\n",
      "        axes       = 2\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1...\n",
      "  [ 1.1119847   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]]),)\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7811e1fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1...\n",
      "  [ 1.1119847   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]]),)\n",
      "        op         = <needle.ops.LogSumExp object at 0x7fc7811e1fd0>\n",
      "        tensor     = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc7811e1a90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[AssertionError('operation needs two equal-sized arrays') raised in repr()] Tensor object at 0x7fc7811e1a90>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:407: in compute\n",
      "    array_api.exp(Z - Z_max_broadcast).sum(axis=\u001b[96mself\u001b[39;49;00m.axes)\n",
      "        Z          = NDArray([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1.463666...7   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]], device=cuda())\n",
      "        Z_max      = NDArray([[ 0.55792385  0.4249462  -0.12920508]\n",
      " [ 0.3461196   1.1605343   0.56536657]\n",
      " [-1.2630658   0.43162832  1.783... 1.0314064   0.9881263 ]\n",
      " [ 0.33515003 -0.17804058  1.1119847 ]\n",
      " [ 0.08052945  0.53987354  0.12419878]], device=cuda())\n",
      "        Z_max_broadcast = NDArray([[[ 0.55792385]\n",
      "  [ 0.4249462 ]\n",
      "  [-0.12920508]]\n",
      "\n",
      " [[ 0.3461196 ]\n",
      "  [ 1.1605343 ]\n",
      "  [ 0.56536657]]\n",
      "\n",
      " [[-1.2630...]]\n",
      "\n",
      " [[ 0.33515003]\n",
      "  [-0.17804058]\n",
      "  [ 1.1119847 ]]\n",
      "\n",
      " [[ 0.08052945]\n",
      "  [ 0.53987354]\n",
      "  [ 0.12419878]]], device=cuda())\n",
      "        self       = <needle.ops.LogSumExp object at 0x7fc7811e1fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:450: in __sub__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\n",
      "        other      = NDArray([[[ 0.55792385]\n",
      "  [ 0.4249462 ]\n",
      "  [-0.12920508]]\n",
      "\n",
      " [[ 0.3461196 ]\n",
      "  [ 1.1605343 ]\n",
      "  [ 0.56536657]]\n",
      "\n",
      " [[-1.2630...]]\n",
      "\n",
      " [[ 0.33515003]\n",
      "  [-0.17804058]\n",
      "  [ 1.1119847 ]]\n",
      "\n",
      " [[ 0.08052945]\n",
      "  [ 0.53987354]\n",
      "  [ 0.12419878]]], device=cuda())\n",
      "        self       = NDArray([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1.463666...7   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]], device=cuda())\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:443: in __add__\n",
      "    \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(\n",
      "        other      = NDArray([[[-0.55792385]\n",
      "  [-0.4249462 ]\n",
      "  [ 0.12920508]]\n",
      "\n",
      " [[-0.3461196 ]\n",
      "  [-1.1605343 ]\n",
      "  [-0.56536657]]\n",
      "\n",
      " [[ 1.2630...]]\n",
      "\n",
      " [[-0.33515003]\n",
      "  [ 0.17804058]\n",
      "  [-1.1119847 ]]\n",
      "\n",
      " [[-0.08052945]\n",
      "  [-0.53987354]\n",
      "  [-0.12419878]]], device=cuda())\n",
      "        self       = NDArray([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1.463666...7   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = NDArray([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1.463666...7   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]], device=cuda())\n",
      "other = NDArray([[[-0.55792385]\n",
      "  [-0.4249462 ]\n",
      "  [ 0.12920508]]\n",
      "\n",
      " [[-0.3461196 ]\n",
      "  [-1.1605343 ]\n",
      "  [-0.56536657]]\n",
      "\n",
      " [[ 1.2630...]]\n",
      "\n",
      " [[-0.33515003]\n",
      "  [ 0.17804058]\n",
      "  [-1.1119847 ]]\n",
      "\n",
      " [[-0.08052945]\n",
      "  [-0.53987354]\n",
      "  [-0.12419878]]], device=cuda())\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc808491630>\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808491660>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mewise_or_scalar\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, other, ewise_func, scalar_func):\n",
      "        \u001b[33m\"\"\"Run either an elementwise or scalar version of a function,\u001b[39;49;00m\n",
      "    \u001b[33m    depending on whether \"other\" is an NDArray or scalar\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "        out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(other, NDArray):\n",
      ">           \u001b[94massert\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.shape == other.shape, \u001b[33m\"\u001b[39;49;00m\u001b[33moperation needs two equal-sized arrays\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           AssertionError: operation needs two equal-sized arrays\u001b[0m\n",
      "\n",
      "ewise_func = <built-in method ewise_add of PyCapsule object at 0x7fc808491630>\n",
      "other      = NDArray([[[-0.55792385]\n",
      "  [-0.4249462 ]\n",
      "  [ 0.12920508]]\n",
      "\n",
      " [[-0.3461196 ]\n",
      "  [-1.1605343 ]\n",
      "  [-0.56536657]]\n",
      "\n",
      " [[ 1.2630...]]\n",
      "\n",
      " [[-0.33515003]\n",
      "  [ 0.17804058]\n",
      "  [-1.1119847 ]]\n",
      "\n",
      " [[-0.08052945]\n",
      "  [-0.53987354]\n",
      "  [-0.12419878]]], device=cuda())\n",
      "out        = NDArray([[[-0.45871726 -0.41777623]\n",
      "  [-1.2863605  -2.7558427 ]\n",
      "  [-0.15774459  0.675676  ]]\n",
      "\n",
      " [[ 1.7372644  -0.797439...2   0.87369984]]\n",
      "\n",
      " [[-1.4355507   0.1892495 ]\n",
      "  [-1.1416229  -0.20496817]\n",
      "  [-0.13278429 -0.44021094]]], device=cuda())\n",
      "scalar_func = <built-in method scalar_add of PyCapsule object at 0x7fc808491660>\n",
      "self       = NDArray([[[ 0.42649978  0.55792385]\n",
      "  [ 0.32932186  0.4249462 ]\n",
      "  [-0.49092507 -0.12920508]]\n",
      "\n",
      " [[ 0.3461196  -1.463666...7   0.9884341 ]]\n",
      "\n",
      " [[-0.05336624  0.08052945]\n",
      "  [ 0.53987354  0.25176743]\n",
      "  [-0.7686626   0.12419878]]], device=cuda())\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:436: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape0]\u001b[0m - AttributeError: 'Tensor' object has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cpu-shape1]\u001b[0m - AttributeError: 'Tensor' object has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape0]\u001b[0m - AttributeError: 'Tensor' object has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape1]\u001b[0m - AttributeError: 'Tensor' object has no attribute 'tanh'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape0-0-1]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape1-0-2]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cpu-shape2-2-5]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape0-0-1]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape1-0-2]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape2-2-5]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape0-0-1]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape1-0-2]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape2-2-5]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape0-0-1]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape1-0-2]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape2-2-5]\u001b[0m - TypeError: 'NoneType' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape0-None]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'ones'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape1-0]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape2-1]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape3-2]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape0-None]\u001b[0m - AttributeError: module 'needle.backend_ndarray' has no attribute 'ones'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape1-0]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape2-1]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape3-2]\u001b[0m - TypeError: 'int' object is not iterable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape0]\u001b[0m - TypeError: 'NoneType' object is not subscriptable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cpu-None-shape1]\u001b[0m - TypeError: 'NoneType' object is not subscriptable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape0]\u001b[0m - TypeError: 'NoneType' object is not subscriptable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape1]\u001b[0m - TypeError: 'NoneType' object is not subscriptable\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape1-0]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape2-1]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cpu-shape3-2]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape1-0]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape2-1]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
      "\u001b[31mFAILED\u001b[0m tests/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape3-2]\u001b[0m - AssertionError: operation needs two equal-sized arrays\n",
      "\u001b[31m================ \u001b[31m\u001b[1m34 failed\u001b[0m, \u001b[32m84 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 4.77s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_nd_backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: CIFAR-10 dataset [10 points]\n",
    "\n",
    "Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images. \n",
    "\n",
    "Start by implementing the `__init__` function in the `CIFAR10Dataset` class. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0 -- /bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/c/SRCs/dlsyscourse/hw4\n",
      "plugins: anyio-3.6.2\n",
      "collected 1803 items / 1791 deselected / 12 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m          [  8%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 16%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 25%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 33%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 41%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 50%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [ 58%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 66%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 75%]\u001b[0m\n",
      "tests/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 83%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n",
      "tests/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] ________\u001b[0m\n",
      "\n",
      "device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        dataloader = ndl.data.DataLoader(\\\n",
      "                 dataset=dataset,\n",
      "                 batch_size=\u001b[94m128\u001b[39;49;00m,\n",
      "                 shuffle=\u001b[94mFalse\u001b[39;49;00m\n",
      "                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\n",
      "                 \u001b[90m# drop_last=False,\u001b[39;49;00m\n",
      "                 \u001b[90m# device=device,\u001b[39;49;00m\n",
      "                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\n",
      "                 )\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      ">       model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "dataloader = <needle.data.DataLoader object at 0x7efc821bf040>\n",
      "dataset    = <needle.data.CIFAR10Dataset object at 0x7efc8217cc40>\n",
      "device     = cpu()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7efc821bf250>, device = cpu()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cpu()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7efc821bf250>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[31m\u001b[1m_______ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] ________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\n",
      "        dataloader = ndl.data.DataLoader(\\\n",
      "                 dataset=dataset,\n",
      "                 batch_size=\u001b[94m128\u001b[39;49;00m,\n",
      "                 shuffle=\u001b[94mFalse\u001b[39;49;00m\n",
      "                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\n",
      "                 \u001b[90m# drop_last=False,\u001b[39;49;00m\n",
      "                 \u001b[90m# device=device,\u001b[39;49;00m\n",
      "                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\n",
      "                 )\n",
      "        \u001b[94mfrom\u001b[39;49;00m \u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m \u001b[94mimport\u001b[39;49;00m ResNet9\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      ">       model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "ResNet9    = <class 'apps.models.ResNet9'>\n",
      "dataloader = <needle.data.DataLoader object at 0x7efd0e450370>\n",
      "dataset    = <needle.data.CIFAR10Dataset object at 0x7efd0e4500a0>\n",
      "device     = cuda()\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:466: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <apps.models.ResNet9 object at 0x7efd0e4501c0>, device = cuda()\n",
      "dtype = 'float32'\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\n",
      "        \u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m()\n",
      "        \u001b[90m### BEGIN YOUR SOLUTION ###\u001b[39;49;00m\n",
      ">       \u001b[94mraise\u001b[39;49;00m \u001b[96mNotImplementedError\u001b[39;49;00m() \u001b[90m###\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       NotImplementedError\u001b[0m\n",
      "\n",
      "__class__  = <class 'apps.models.ResNet9'>\n",
      "device     = cuda()\n",
      "dtype      = 'float32'\n",
      "self       = <apps.models.ResNet9 object at 0x7efd0e4501c0>\n",
      "\n",
      "\u001b[1m\u001b[31mapps/models.py\u001b[0m:14: NotImplementedError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - NotImplementedError\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - NotImplementedError\n",
      "\u001b[31m================ \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[32m10 passed\u001b[0m, \u001b[33m1791 deselected\u001b[0m\u001b[31m in 14.77s\u001b[0m\u001b[31m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"cifar10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"cifar10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Convolutional neural network [40 points]\n",
    "\n",
    "Here's an outline of what you will do in this task.\n",
    "\n",
    "In `python/needle/backend_ndarray/ndarray.py`, implement:\n",
    "- `flip`\n",
    "- `pad`\n",
    "\n",
    "In `python/needle/ops.py`, implement (forward and backward):\n",
    "- `Flip`\n",
    "- `Dilate`\n",
    "- `UnDilate`\n",
    "- `Conv`\n",
    "\n",
    "In `python/needle/nn.py`, implement:\n",
    "- `Flatten`\n",
    "- `Conv`\n",
    "\n",
    "In `python/apps/models.py`, fill in the `ResNet9` class.  \n",
    "\n",
    "In `apps/simple_training.py`, fill in:\n",
    "- `epoch_general_cifar10`,\n",
    "- `train_cifar10`\n",
    "- `evaluate_cifar10`\n",
    "\n",
    "We have provided a `BatchNorm2d` implementation for you as a wrapper around your previous `BatchNorm1d` implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding ndarrays\n",
    "\n",
    "Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n",
    "e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n",
    "A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3). \n",
    "\n",
    "Padding is also required for the backward pass of convolution.\n",
    "\n",
    "You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n",
    "That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n",
    "where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n",
    "\n",
    "For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0 -- /bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/c/SRCs/dlsyscourse/hw4\n",
      "plugins: anyio-3.6.2\n",
      "collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 1.31s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"pad_forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping ndarrays & FlipOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n",
    "# i.e., ignoring strides\n",
    "def raw_data(X):\n",
    "    X = np.array(X) # copy, thus compact X\n",
    "    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n",
    "\n",
    "# Xold and Xnew should reference the same underlying data\n",
    "def offset(Xold, Xnew):\n",
    "    assert Xold.itemsize == Xnew.itemsize\n",
    "    # compare addresses to the beginning of the arrays\n",
    "    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n",
    "\n",
    "def strides(X):\n",
    "    return ', '.join([str(x//X.itemsize) for x in X.strides])\n",
    "\n",
    "def format_array(X, shape):\n",
    "    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n",
    "    def chunks(l, n):\n",
    "        n = max(1, n)\n",
    "        return (l[i:i+n] for i in range(0, len(l), n))\n",
    "    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n",
    "    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n",
    "    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n",
    "    return '  '.join(a)\n",
    "\n",
    "def inspect_array(X, *, is_a_copy_of):\n",
    "    # compacts X, then reads it off in order\n",
    "    print('Data: %s' % format_array(raw_data(X), X.shape))\n",
    "    # compares address of X to copy_of, thus finding X's offset\n",
    "    print('Offset: %s' % offset(is_a_copy_of, X))\n",
    "    print('Strides: %s' % strides(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n",
    "axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n",
    "function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n",
    "\n",
    "We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n",
    "`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n",
    "\n",
    "For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n",
    "\n",
    "Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n",
    "\n",
    "Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this array as reference for the other examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(1, 25).reshape(3, 2, 4)\n",
    "inspect_array(A, is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when you flip the array along the last axis below. \n",
    "Note that the `inspect_array` function compacts the array after flipping it so you can see the\n",
    "\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n",
    "flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n",
    "\n",
    "That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n",
    "to copy this behavior in our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to infer the more general algorithm for computing the offset given the axis to flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe what happens when we flip _all_ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1,2)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flip just axes 1 and 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_array(np.flip(A, (0,1)), is_a_copy_of=A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset is 20. Looking back on our previous offset computations, do you notice something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n",
    "try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops.py`; note that these should be extremely short.\n",
    "\n",
    "**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n",
    "\n",
    "Also, if you want to instead add a `flip` operator on the CPU/CUDA backends, that's also okay.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0 -- /bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /mnt/c/SRCs/dlsyscourse/hw4\n",
      "plugins: anyio-3.6.2\n",
      "collected 1803 items / 1763 deselected / 40 selected                           \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 10%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 12%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 22%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 25%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 27%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 30%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 37%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 40%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 45%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 95%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n",
      "tests/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028],\n",
      "       [-1.04855297, -1.42001794, -1.7062701...842, -0.15135721, -0.10321885,  0.4105985 ],\n",
      "       [ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799]])\n",
      "axes       = (0,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd071095040>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd071095040>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cpu.Array object at 0x7fd070f51270>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd0710950d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cpu.Array object at 0x7fd070f518b0>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd0710950d0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd071095040>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cpu.Array object at 0x7fd071217b30>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd071095160>\n",
      "        axes       = (0,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 45\n",
      "        new_strides = [-5, 1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cpu.Array object at 0x7fd071217b30>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd071095160>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd071217b30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd0710840f0>, (10, 5), (-5, 1), 45\u001b[0m\n",
      "\n",
      "out        = NDArray([[1.07867011e+30 4.58504858e-41 1.07867857e+30 4.58504858e-41\n",
      "  1.07868703e+30]\n",
      " [4.58504858e-41 1.07869549e+3...8e-41\n",
      "  1.07885628e+30]\n",
      " [4.58504858e-41 1.04451674e+30 4.58504858e-41 1.04452521e+30\n",
      "  4.58504858e-41]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cpu.Array object at 0x7fd071217b30>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd071095160>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028],\n",
      "       [-1.04855297, -1.42001794, -1.7062701...842, -0.15135721, -0.10321885,  0.4105985 ],\n",
      "       [ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799]])\n",
      "axes       = (0,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e3cb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e3cb50>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cuda.Array object at 0x7fd070e8c5b0>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd070e3c9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cuda.Array object at 0x7fd070e8c530>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd070e3c9a0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e3cb50>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cuda.Array object at 0x7fd0fd563530>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd070e3ca60>\n",
      "        axes       = (0,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 45\n",
      "        new_strides = [-5, 1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cuda.Array object at 0x7fd0fd563530>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd070e3ca60>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd0fd563530>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070dac4b0>, (10, 5), (-5, 1), 45\u001b[0m\n",
      "\n",
      "out        = NDArray([[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cuda.Array object at 0x7fd0fd563530>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd070e3ca60>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235],\n",
      "       [ 0.4105985 , -0.10321885, -0.1513572...54 , -1.70627019, -1.42001794, -1.04855297],\n",
      "       [-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ]])\n",
      "axes       = (1,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd068040cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd068040cd0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...y_backend_cpu.Array object at 0x7fd068030170>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd068040dc0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...y_backend_cpu.Array object at 0x7fd070e5d230>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd068040dc0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd068040cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n..._backend_cpu.Array object at 0x7fd068060df0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd068040f10>\n",
      "        axes       = (1,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 4\n",
      "        new_strides = [5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n..._backend_cpu.Array object at 0x7fd068060df0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd068040f10>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd068060df0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd0680606b0>, (10, 5), (5, -1), 4\u001b[0m\n",
      "\n",
      "out        = NDArray([[6.0228563e+29 4.5850486e-41 6.0256973e+29 4.5850486e-41 6.0257155e+29]\n",
      " [4.5850486e-41 6.0257336e+29 4.58504...9 4.5850486e-41 6.0260781e+29]\n",
      " [4.5850486e-41 6.0260963e+29 4.5850486e-41 6.0261144e+29 4.5850486e-41]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n..._backend_cpu.Array object at 0x7fd068060df0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd068040f10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235],\n",
      "       [ 0.4105985 , -0.10321885, -0.1513572...54 , -1.70627019, -1.42001794, -1.04855297],\n",
      "       [-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ]])\n",
      "axes       = (1,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d60160>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d60160>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cuda.Array object at 0x7fd070eec1f0>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd070d60280>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cuda.Array object at 0x7fd070eece70>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd070d60280>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d60160>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cuda.Array object at 0x7fd070e5d9f0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd070d60310>\n",
      "        axes       = (1,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 4\n",
      "        new_strides = [5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cuda.Array object at 0x7fd070e5d9f0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd070d60310>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070e5d9f0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070e5d8b0>, (10, 5), (5, -1), 4\u001b[0m\n",
      "\n",
      "out        = NDArray([[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cuda.Array object at 0x7fd070e5d9f0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd070d60310>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ],\n",
      "       [-0.50965218,  1.9507754 , -1.7062701...885, -0.15135721,  0.95008842, -0.97727788],\n",
      "       [ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235]])\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b5a850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd060b5a850>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cpu.Array object at 0x7fd070d94ab0>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd060b5a8e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cpu.Array object at 0x7fd070d94cb0>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd060b5a8e0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b5a850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cpu.Array object at 0x7fd068050d30>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd060b5a970>\n",
      "        axes       = (0, 1)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 49\n",
      "        new_strides = [-5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cpu.Array object at 0x7fd068050d30>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd060b5a970>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd068050d30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd0680506b0>, (10, 5), (-5, -1), 49\u001b[0m\n",
      "\n",
      "out        = NDArray([[3.2575667e-15 4.5851887e-41 3.2575667e-15 4.5851887e-41 3.2575667e-15]\n",
      " [4.5851887e-41 3.2575667e-15 4.58518...5 4.5851887e-41 6.8019365e+29]\n",
      " [4.5850486e-41 3.2575667e-15 4.5851887e-41 3.2575667e-15 4.5851887e-41]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cpu.Array object at 0x7fd068050d30>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd060b5a970>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "_A         = array([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\n",
      "       [-0.97727788,  0.95008842, -0.1513572...794, -1.70627019,  1.9507754 , -0.50965218],\n",
      "       [-0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n",
      "_B         = array([[-0.21274028, -1.61389785,  0.77749036, -1.25279536, -0.4380743 ],\n",
      "       [-0.50965218,  1.9507754 , -1.7062701...885, -0.15135721,  0.95008842, -0.97727788],\n",
      "       [ 1.86755799,  2.2408932 ,  0.97873798,  0.40015721,  1.76405235]])\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d9eee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d9eee0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ackend_cuda.Array object at 0x7fd070e2b630>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd070d9ef40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ackend_cuda.Array object at 0x7fd070e2ba70>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd070d9ef40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d9eee0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ckend_cuda.Array object at 0x7fd070e2cf30>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070d9efa0>\n",
      "        axes       = (0, 1)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 49\n",
      "        new_strides = [-5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ckend_cuda.Array object at 0x7fd070e2cf30>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070d9efa0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070e2cf30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070e2c170>, (10, 5), (-5, -1), 49\u001b[0m\n",
      "\n",
      "out        = NDArray([[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ckend_cuda.Array object at 0x7fd070e2cf30>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070d9efa0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 1.67386460e+00,  1.09525980e+00, -4.17631359e-01, ...,\n",
      "           9.36113484e-01,  5.84919249e-01, -8.18787...8.13364259e-01, -1.46642433e+00,  5.21064876e-01, ...,\n",
      "          -3.19328417e-01,  6.91538751e-01,  6.94749144e-01]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd06801b190>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd06801b190>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ct at 0x7fd070d410f0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] Tensor object at 0x7fd06801b1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ct at 0x7fd070d41570>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] Tensor object at 0x7fd06801b1f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd06801b190>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...t at 0x7fd06803b4b0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] NDArray object at 0x7fd06801b3a0>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 81664\n",
      "        new_strides = [-8192, -256, 8, 1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...t at 0x7fd06803b4b0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] NDArray object at 0x7fd06801b3a0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd06803b4b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd06803b370>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[-6.72460437e-01 -3.59553158e-01 -8.13146293e-01 ... -4.01780933e-01\n",
      "    -1.63019836e+00  4.62782264e-01]\n",
      " ...00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "     0.00000000e+00  0.00000000e+00]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...t at 0x7fd06803b4b0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] NDArray object at 0x7fd06801b3a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 1.67386460e+00,  1.09525980e+00, -4.17631359e-01, ...,\n",
      "           9.36113484e-01,  5.84919249e-01, -8.18787...8.13364259e-01, -1.46642433e+00,  5.21064876e-01, ...,\n",
      "          -3.19328417e-01,  6.91538751e-01,  6.94749144e-01]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d9b760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d9b760>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ct at 0x7fd060b670f0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] Tensor object at 0x7fd070d9b7f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ct at 0x7fd060b678b0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] Tensor object at 0x7fd070d9b7f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d9b760>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...t at 0x7fd070d2f370>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] NDArray object at 0x7fd070d9b880>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 81664\n",
      "        new_strides = [-8192, -256, 8, 1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...t at 0x7fd070d2f370>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] NDArray object at 0x7fd070d9b880>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d2f370>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d2f5b0>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...t at 0x7fd070d2f370>, (10, 32, 32, 8), (-8192, -256, 8, 1), 81664') raised in repr()] NDArray object at 0x7fd070d9b880>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.03863055, -1.6567151 , -0.98551074, -1.47183501,\n",
      "           1.64813493,  0.16422776,  0.56729028, -0.2226... [-1.04855297, -1.42001794, -1.70627019,  1.9507754 ,\n",
      "          -0.50965218, -0.4380743 , -1.25279536,  0.77749036]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd068069af0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd068069af0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd070d7e170>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] Tensor object at 0x7fd068069c40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd070d7e0b0>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] Tensor object at 0x7fd068069c40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd068069af0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd060b64cb0>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] NDArray object at 0x7fd068069d60>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 384\n",
      "        new_strides = [-144, -48, 8, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd060b64cb0>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] NDArray object at 0x7fd068069d60>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd060b64cb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd060b641b0>, (3, 3, 6, 8), (-144, -48, 8, 1), 384\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[4.23192136e-43 0.00000000e+00 1.09206661e+20 4.58504858e-41\n",
      "    1.09168381e+20 4.58504858e-41 1.09188647e+...58e-41 1.07540330e+20 4.58504858e-41\n",
      "    1.07542581e+20 4.58504858e-41 1.07545396e+20 4.58504858e-41]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd060b64cb0>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] NDArray object at 0x7fd068069d60>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.03863055, -1.6567151 , -0.98551074, -1.47183501,\n",
      "           1.64813493,  0.16422776,  0.56729028, -0.2226... [-1.04855297, -1.42001794, -1.70627019,  1.9507754 ,\n",
      "          -0.50965218, -0.4380743 , -1.25279536,  0.77749036]]]])\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd068081df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd068081df0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd060b1d270>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] Tensor object at 0x7fd068081f40>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd060b1d130>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] Tensor object at 0x7fd068081f40>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd068081df0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd060b55830>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] NDArray object at 0x7fd068081e50>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 384\n",
      "        new_strides = [-144, -48, 8, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd060b55830>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] NDArray object at 0x7fd068081e50>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd060b55830>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd060b55070>, (3, 3, 6, 8), (-144, -48, 8, 1), 384\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0... 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd060b55830>, (3, 3, 6, 8), (-144, -48, 8, 1), 384') raised in repr()] NDArray object at 0x7fd068081e50>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.85915746e-01,  1.85270697e+00,  1.58169123e+00, ...,\n",
      "           6.66044853e-01,  5.32038766e-01, -1.05080...1.02348043e+00,  8.92468343e-01,  9.79886230e-01, ...,\n",
      "          -1.92371106e+00, -1.58578942e+00, -8.67523255e-02]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070df2400>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070df2400>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ect at 0x7fd070d28530>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] Tensor object at 0x7fd070df2460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ect at 0x7fd070d289f0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] Tensor object at 0x7fd070df2460>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070df2400>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ct at 0x7fd070de0db0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] NDArray object at 0x7fd070df25b0>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 8184\n",
      "        new_strides = [8192, -256, -8, 1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ct at 0x7fd070de0db0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] NDArray object at 0x7fd070df25b0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070de0db0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070de07b0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[ 1.53277922e+00  1.46935880e+00  1.54947430e-01 ... -1.98079646e+00\n",
      "    -3.47912163e-01  1.56348974e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ct at 0x7fd070de0db0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] NDArray object at 0x7fd070df25b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.85915746e-01,  1.85270697e+00,  1.58169123e+00, ...,\n",
      "           6.66044853e-01,  5.32038766e-01, -1.05080...1.02348043e+00,  8.92468343e-01,  9.79886230e-01, ...,\n",
      "          -1.92371106e+00, -1.58578942e+00, -8.67523255e-02]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070da0760>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070da0760>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ect at 0x7fd0680584b0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] Tensor object at 0x7fd070da0070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ect at 0x7fd068058930>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] Tensor object at 0x7fd070da0070>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070da0760>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ct at 0x7fd070d390b0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] NDArray object at 0x7fd070da08b0>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 8184\n",
      "        new_strides = [8192, -256, -8, 1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ct at 0x7fd070d390b0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] NDArray object at 0x7fd070da08b0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d390b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d39470>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ct at 0x7fd070d390b0>, (10, 32, 32, 8), (8192, -256, -8, 1), 8184') raised in repr()] NDArray object at 0x7fd070da08b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.57659082, -0.20829876,  0.39600671, -1.09306151,\n",
      "          -1.49125759,  0.4393917 ,  0.1666735 ,  0.6350... [ 0.68981816,  1.30184623, -0.62808756, -0.48102712,\n",
      "           2.3039167 , -1.06001582, -0.1359497 ,  1.13689136]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b0ed30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd060b0ed30>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd060b561b0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] Tensor object at 0x7fd060b0eeb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd060b56470>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] Tensor object at 0x7fd060b0eeb0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b0ed30>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070ec9fb0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] NDArray object at 0x7fd060b0efd0>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 136\n",
      "        new_strides = [144, -48, -8, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070ec9fb0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] NDArray object at 0x7fd060b0efd0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070ec9fb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070ec9a70>, (3, 3, 6, 8), (144, -48, -8, 1), 136\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[ 5.3975202e+29  4.5850486e-41  0.0000000e+00  0.0000000e+00\n",
      "     1.6395192e-43  5.6051939e-45  1.4012985e-...39e-45  1.4012985e-45  0.0000000e+00\n",
      "     5.9795768e+29  4.5850486e-41  6.7822846e-43            nan]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070ec9fb0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] NDArray object at 0x7fd060b0efd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.57659082, -0.20829876,  0.39600671, -1.09306151,\n",
      "          -1.49125759,  0.4393917 ,  0.1666735 ,  0.6350... [ 0.68981816,  1.30184623, -0.62808756, -0.48102712,\n",
      "           2.3039167 , -1.06001582, -0.1359497 ,  1.13689136]]]])\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d910d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d910d0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd070d2dfb0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] Tensor object at 0x7fd070d91070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd070d2dfb0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] Tensor object at 0x7fd070d91070>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d910d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070d815b0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] NDArray object at 0x7fd070d911f0>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 136\n",
      "        new_strides = [144, -48, -8, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070d815b0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] NDArray object at 0x7fd070d911f0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d815b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d818f0>, (3, 3, 6, 8), (144, -48, -8, 1), 136\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0... 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070d815b0>, (3, 3, 6, 8), (144, -48, -8, 1), 136') raised in repr()] NDArray object at 0x7fd070d911f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.94749144e-01,  6.91538751e-01, -3.19328417e-01, ...,\n",
      "           5.21064876e-01, -1.46642433e+00, -8.13364...8.18787218e-01,  5.84919249e-01,  9.36113484e-01, ...,\n",
      "          -4.17631359e-01,  1.09525980e+00,  1.67386460e+00]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d54730>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d54730>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ject at 0x7fd070ecf3b0>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] Tensor object at 0x7fd070d54070>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ject at 0x7fd070ecf770>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] Tensor object at 0x7fd070d54070>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d54730>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ect at 0x7fd070f31530>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] NDArray object at 0x7fd070d541f0>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 255\n",
      "        new_strides = [8192, 256, -8, -1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ect at 0x7fd070f31530>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] NDArray object at 0x7fd070d541f0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070f31530>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070f31630>, (10, 32, 32, 8), (8192, 256, -8, -1), 255\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[ 1.53277922e+00  1.46935880e+00  1.54947430e-01 ... -1.98079646e+00\n",
      "    -3.47912163e-01  1.56348974e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ect at 0x7fd070f31530>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] NDArray object at 0x7fd070d541f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 6.94749144e-01,  6.91538751e-01, -3.19328417e-01, ...,\n",
      "           5.21064876e-01, -1.46642433e+00, -8.13364...8.18787218e-01,  5.84919249e-01,  9.36113484e-01, ...,\n",
      "          -4.17631359e-01,  1.09525980e+00,  1.67386460e+00]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b382b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd060b382b0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ject at 0x7fd070de8170>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] Tensor object at 0x7fd060b38310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ject at 0x7fd070de83b0>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] Tensor object at 0x7fd060b38310>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b382b0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ect at 0x7fd068068fb0>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] NDArray object at 0x7fd060b384f0>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 255\n",
      "        new_strides = [8192, 256, -8, -1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ect at 0x7fd068068fb0>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] NDArray object at 0x7fd060b384f0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd068068fb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd068068070>, (10, 32, 32, 8), (8192, 256, -8, -1), 255\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ect at 0x7fd068068fb0>, (10, 32, 32, 8), (8192, 256, -8, -1), 255') raised in repr()] NDArray object at 0x7fd060b384f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.77749036, -1.25279536, -0.4380743 , -0.50965218,\n",
      "           1.9507754 , -1.70627019, -1.42001794, -1.0485... [-0.2226751 ,  0.56729028,  0.16422776,  1.64813493,\n",
      "          -1.47183501, -0.98551074, -1.6567151 ,  0.03863055]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd06806a8b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd06806a8b0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd07109c670>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] Tensor object at 0x7fd06806aac0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd07109c130>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] Tensor object at 0x7fd06806aac0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd06806a8b0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070df0f70>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] NDArray object at 0x7fd06806aaf0>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 47\n",
      "        new_strides = [144, 48, -8, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070df0f70>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] NDArray object at 0x7fd06806aaf0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070df0f70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070df0cb0>, (3, 3, 6, 8), (144, 48, -8, -1), 47\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[1.75681795e-04 1.30285140e-11 1.45852415e-19 2.53488541e-09\n",
      "    1.75663110e-04 5.37782443e+22 2.51799558e-...28e-11 1.45852415e-19 1.66140962e-04\n",
      "    1.74735571e-04 5.31856381e+22 2.51799558e-12 3.96039868e-11]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070df0f70>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] NDArray object at 0x7fd06806aaf0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "_A         = array([[[[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,\n",
      "           1.86755799, -0.97727788,  0.95008842, -0.1513... [-0.13010695,  0.09395323,  0.94304609, -2.73967717,\n",
      "          -0.56931205,  0.26990435, -0.46684555, -1.41690611]]]])\n",
      "_B         = array([[[[ 0.77749036, -1.25279536, -0.4380743 , -0.50965218,\n",
      "           1.9507754 , -1.70627019, -1.42001794, -1.0485... [-0.2226751 ,  0.56729028,  0.16422776,  1.64813493,\n",
      "          -1.47183501, -0.98551074, -1.6567151 ,  0.03863055]]]])\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 8)}\n",
      "shape      = (3, 3, 6, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ....12372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070ddb9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...2372191]\n",
      "   [-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070ddb9d0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd071635ab0>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] Tensor object at 0x7fd070ddbee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd071635ab0>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] Tensor object at 0x7fd070ddbee0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070ddb9d0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070f1bcf0>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] NDArray object at 0x7fd070ddbf10>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 47\n",
      "        new_strides = [144, 48, -8, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....-0.13010696  0.09395323  0.9430461  -2.7396772  -0.56931204\n",
      "     0.26990435 -0.46684554 -1.4169061 ]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070f1bcf0>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] NDArray object at 0x7fd070ddbf10>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070f1bcf0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070f1b1f0>, (3, 3, 6, 8), (144, 48, -8, -1), 47\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0... 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070f1bcf0>, (3, 3, 6, 8), (144, 48, -8, -1), 47') raised in repr()] NDArray object at 0x7fd070ddbf10>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 8.28751864e-01,  6.47998581e-02,  3.40539329e-01, ...,\n",
      "           1.22084664e+00, -2.06575306e-01,  4.08956...1.51357208e-01,  9.50088418e-01, -9.77277880e-01, ...,\n",
      "           9.78737984e-01,  4.00157208e-01,  1.76405235e+00]]]])\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070eee310>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070eee310>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne... at 0x7fd070d585b0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] Tensor object at 0x7fd070eee250>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne... at 0x7fd070d58a70>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] Tensor object at 0x7fd070eee250>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070eee310>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...at 0x7fd070e494f0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] NDArray object at 0x7fd070eee3d0>\n",
      "        axes       = (0, 1, 2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 81919\n",
      "        new_strides = [-8192, -256, -8, -1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...at 0x7fd070e494f0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] NDArray object at 0x7fd070eee3d0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070e494f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070e49d70>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[ 1.53277922e+00  1.46935880e+00  1.54947430e-01 ... -1.98079646e+00\n",
      "    -3.47912163e-01  1.56348974e-01]\n",
      " ...08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...at 0x7fd070e494f0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] NDArray object at 0x7fd070eee3d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_forward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_forward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        _A = np.random.randn(*shape)\n",
      "        _B = np.flip(_A, axes)\n",
      "        A = ndl.Tensor(_A, device=device)\n",
      ">       B = ndl.flip(A, axes=axes)\n",
      "\n",
      "A          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "_A         = array([[[[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01, ...,\n",
      "          -9.77277880e-01,  9.50088418e-01, -1.51357...4.08956722e-01, -2.06575306e-01,  1.22084664e+00, ...,\n",
      "           3.40539329e-01,  6.47998581e-02,  8.28751864e-01]]]])\n",
      "_B         = array([[[[ 8.28751864e-01,  6.47998581e-02,  3.40539329e-01, ...,\n",
      "           1.22084664e+00, -2.06575306e-01,  4.08956...1.51357208e-01,  9.50088418e-01, -9.77277880e-01, ...,\n",
      "           9.78737984e-01,  4.00157208e-01,  1.76405235e+00]]]])\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (10, 32, 32, 8)}\n",
      "shape      = (10, 32, 32, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:123: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e...7e+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e7e7c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204...+00]\n",
      "   [ 4.08956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e7e7c0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne... at 0x7fd060b3b1f0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] Tensor object at 0x7fd070e7e880>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne... at 0x7fd060b3b8b0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] Tensor object at 0x7fd070e7e880>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e7e7c0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...at 0x7fd060bc2bb0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] NDArray object at 0x7fd070e7e7f0>\n",
      "        axes       = (0, 1, 2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 81919\n",
      "        new_strides = [-8192, -256, -8, -1]\n",
      "        self       = NDArray([[[[ 1.76405239e+00  4.00157213e-01  9.78738010e-01 ... -9.77277875e-01\n",
      "     9.50088441e-01 -1.51357204e-01]\n",
      " ...8956736e-01 -2.06575304e-01  1.22084665e+00 ...  3.40539336e-01\n",
      "     6.47998601e-02  8.28751862e-01]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...at 0x7fd060bc2bb0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] NDArray object at 0x7fd070e7e7f0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd060bc2bb0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd060bc2c30>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...at 0x7fd060bc2bb0>, (10, 32, 32, 8), (-8192, -256, -8, -1), 81919') raised in repr()] NDArray object at 0x7fd070e7e7f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e24cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e24cd0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cpu.Array object at 0x7fd068047470>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd070e24d60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cpu.Array object at 0x7fd068047130>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd070e24d60>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e24cd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cpu.Array object at 0x7fd060bd6cb0>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd070e24dc0>\n",
      "        axes       = (0,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 45\n",
      "        new_strides = [-5, 1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cpu.Array object at 0x7fd060bd6cb0>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd070e24dc0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd060bd6cb0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd060bd6a70>, (10, 5), (-5, 1), 45\u001b[0m\n",
      "\n",
      "out        = NDArray([[6.89135867e+34 3.90983535e+30 1.34578644e-14 1.72544590e+19\n",
      "  2.93959644e+29]\n",
      " [6.45286927e-04 4.39790014e+2...8e+25\n",
      "  1.65815419e+25]\n",
      " [3.14706204e-12 1.14460433e+24 4.20026320e+07 6.88067891e+16\n",
      "  1.10082067e+18]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cpu.Array object at 0x7fd060bd6cb0>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd070e24dc0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b3d0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd060b3d0a0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cuda.Array object at 0x7fd070ec03f0>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd060b3d220>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cuda.Array object at 0x7fd070dc7b30>, (10, 5), (-5, 1), 45') raised in repr()] Tensor object at 0x7fd060b3d220>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b3d0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        axes       = (0,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cuda.Array object at 0x7fd070eed870>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd060b3d2b0>\n",
      "        axes       = (0,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 45\n",
      "        new_strides = [-5, 1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cuda.Array object at 0x7fd070eed870>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd060b3d2b0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070eed870>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070eed230>, (10, 5), (-5, 1), 45\u001b[0m\n",
      "\n",
      "out        = NDArray([[ 0.67229474  0.40746182 -0.76991606  0.5392492  -0.6743327 ]\n",
      " [ 0.03183056 -0.6358461   0.67643327  0.576590...6   0.9208588   0.31872764  0.8568306 ]\n",
      " [-0.6510256  -1.0342429   0.6815945  -0.80340964 -0.6895498 ]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cuda.Array object at 0x7fd070eed870>, (10, 5), (-5, 1), 45') raised in repr()] NDArray object at 0x7fd060b3d2b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (1,)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (1,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd06806c8b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd06806c8b0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...y_backend_cpu.Array object at 0x7fd070dd0930>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd06806c910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...y_backend_cpu.Array object at 0x7fd070dd0b30>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd06806c910>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd06806c8b0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n..._backend_cpu.Array object at 0x7fd070d413f0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd06806c9a0>\n",
      "        axes       = (1,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 4\n",
      "        new_strides = [5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n..._backend_cpu.Array object at 0x7fd070d413f0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd06806c9a0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070d413f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070d41bb0>, (10, 5), (5, -1), 4\u001b[0m\n",
      "\n",
      "out        = NDArray([[1.3730931e-38 0.0000000e+00 2.9287138e-43 0.0000000e+00 3.5647647e-35]\n",
      " [0.0000000e+00 6.9714877e-15 4.58518...9 4.5850486e-41 5.3394929e+29]\n",
      " [4.5850486e-41 5.3398072e+29 4.5850486e-41 5.3398193e+29 4.5850486e-41]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n..._backend_cpu.Array object at 0x7fd070d413f0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd06806c9a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1,), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (1,)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1,), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (1,)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e658e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e658e0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cuda.Array object at 0x7fd070f514b0>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd070e65940>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne..._backend_cuda.Array object at 0x7fd070eff5f0>, (10, 5), (5, -1), 4') raised in repr()] Tensor object at 0x7fd070e65940>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e658e0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        axes       = (1,)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cuda.Array object at 0x7fd0680222b0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd070e659d0>\n",
      "        axes       = (1,)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 4\n",
      "        new_strides = [5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cuda.Array object at 0x7fd0680222b0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd070e659d0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd0680222b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd068022330>, (10, 5), (5, -1), 4\u001b[0m\n",
      "\n",
      "out        = NDArray([[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...backend_cuda.Array object at 0x7fd0680222b0>, (10, 5), (5, -1), 4') raised in repr()] NDArray object at 0x7fd070e659d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070da2bb0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070da2bb0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cpu.Array object at 0x7fd06804aa30>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd070da2c70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...backend_cpu.Array object at 0x7fd070dede30>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd070da2c70>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070da2bb0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cpu.Array object at 0x7fd070ec14f0>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070da2cd0>\n",
      "        axes       = (0, 1)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 49\n",
      "        new_strides = [-5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...8   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cpu.Array object at 0x7fd070ec14f0>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070da2cd0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070ec14f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070ec16f0>, (10, 5), (-5, -1), 49\u001b[0m\n",
      "\n",
      "out        = NDArray([[5.8343606e+29 4.5850486e-41 5.8347958e+29 4.5850486e-41 5.8339980e+29]\n",
      " [4.5850486e-41 5.8348502e+29 4.58504...9 4.5850486e-41 5.7675312e+29]\n",
      " [4.5850486e-41 5.7687824e+29 4.5850486e-41 3.2575667e-15 4.5851887e-41]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ackend_cpu.Array object at 0x7fd070ec14f0>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070da2cd0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (10, 5)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (10, 5)}\n",
      "shape      = (10, 5)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0....553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070f25f70>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0...3   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070f25f70>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ackend_cuda.Array object at 0x7fd070efcef0>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd070f25fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ackend_cuda.Array object at 0x7fd070efc030>, (10, 5), (-5, -1), 49') raised in repr()] Tensor object at 0x7fd070f25fd0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070f25f70>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ckend_cuda.Array object at 0x7fd070dd4a70>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070f250a0>\n",
      "        axes       = (0, 1)\n",
      "        i          = 1\n",
      "        n          = 2\n",
      "        new_offset = 49\n",
      "        new_strides = [-5, -1]\n",
      "        self       = NDArray([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.103218...   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ckend_cuda.Array object at 0x7fd070dd4a70>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070f250a0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070dd4a70>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070dd46f0>, (10, 5), (-5, -1), 49\u001b[0m\n",
      "\n",
      "out        = NDArray([[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ckend_cuda.Array object at 0x7fd070dd4a70>, (10, 5), (-5, -1), 49') raised in repr()] NDArray object at 0x7fd070f250a0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}\n",
      "shape      = (2, 3, 3, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ...0.67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d3f3a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d3f3a0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd070e9d930>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] Tensor object at 0x7fd070d3f460>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd070e9d1b0>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] Tensor object at 0x7fd070d3f460>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0.... [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d3f3a0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0.... [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cpu())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070e63630>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] NDArray object at 0x7fd070d3f520>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 120\n",
      "        new_strides = [-72, -24, 8, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0.... [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070e63630>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] NDArray object at 0x7fd070d3f520>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070e63630>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070e63070>, (2, 3, 3, 8), (-72, -24, 8, 1), 120\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[1.08376988e+20 4.58504858e-41 0.00000000e+00 0.00000000e+00\n",
      "    5.36697312e-43 1.12103877e-44 1.40129846e-...78e-44 1.40129846e-45 4.58518871e-41\n",
      "    1.02018635e+20 4.58504858e-41 2.67648007e-43 7.98740125e-44]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070e63630>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] NDArray object at 0x7fd070d3f520>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (2, 3, 3, 8)}\n",
      "shape      = (2, 3, 3, 8)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      " ...0.67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070f3fd90>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "...67643327]\n",
      "   [ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070f3fd90>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd060b7a770>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] Tensor object at 0x7fd070f3fe20>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd060b7a4b0>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] Tensor object at 0x7fd070f3fe20>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070f3fd90>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cuda())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070ecca30>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] NDArray object at 0x7fd070f3fe80>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 120\n",
      "        new_strides = [-72, -24, 8, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n",
      "    -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [-0....[ 0.57659084 -0.20829876  0.3960067  -1.0930616  -1.4912575\n",
      "     0.4393917   0.1666735   0.63503146]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070ecca30>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] NDArray object at 0x7fd070f3fe80>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070ecca30>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070ecc730>, (2, 3, 3, 8), (-72, -24, 8, 1), 120\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. 0..... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070ecca30>, (2, 3, 3, 8), (-72, -24, 8, 1), 120') raised in repr()] NDArray object at 0x7fd070f3fe80>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e341c0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e341c0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd070dfe230>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] Tensor object at 0x7fd070e342e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd070dfe2f0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] Tensor object at 0x7fd070e342e0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e341c0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd071083930>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] NDArray object at 0x7fd070e34370>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 192\n",
      "        new_strides = [-72, -24, 4, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd071083930>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] NDArray object at 0x7fd070e34370>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd071083930>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd0710836b0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[3.1480047e-15 4.5851887e-41 3.1480047e-15 4.5851887e-41]\n",
      "   [3.1480047e-15 4.5851887e-41 3.1480047e-15 4.5...5851887e-41 3.1480047e-15 4.5851887e-41]\n",
      "   [3.1480047e-15 4.5851887e-41 3.1480047e-15 4.5851887e-41]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd071083930>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] NDArray object at 0x7fd070e34370>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e0ffd0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e0ffd0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd060bde970>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] Tensor object at 0x7fd070e0fee0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ray object at 0x7fd060bdecb0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] Tensor object at 0x7fd070e0fee0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e0ffd0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "        axes       = (0, 1)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070f395b0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] NDArray object at 0x7fd070e0fdc0>\n",
      "        axes       = (0, 1)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 192\n",
      "        new_strides = [-72, -24, 4, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070f395b0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] NDArray object at 0x7fd070e0fdc0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070f395b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070f39eb0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]\n",
      "\n",
      "  [[0.... [[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ay object at 0x7fd070f395b0>, (3, 3, 6, 4), (-72, -24, 4, 1), 192') raised in repr()] NDArray object at 0x7fd070e0fdc0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d3b0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d3b0a0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070dc31f0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] Tensor object at 0x7fd070d3b130>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070dc31f0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] Tensor object at 0x7fd070d3b130>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d3b0a0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070eb88b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] NDArray object at 0x7fd070d3bc70>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 32\n",
      "        new_strides = [36, -12, -4, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070eb88b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] NDArray object at 0x7fd070d3bc70>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070eb88b0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070eb85b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0.00000000e+00 0.00000000e+00 4.82046672e-43            nan]\n",
      "   [1.06647605e+20 4.58504858e-41 0.00000000e...8e-41 0.00000000e+00 0.00000000e+00]\n",
      "   [5.42302506e-43 1.12103877e-44 1.40129846e-45 0.00000000e+00]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070eb88b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] NDArray object at 0x7fd070d3bc70>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070f0f910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070f0f910>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070e9b870>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] Tensor object at 0x7fd070f0f970>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070e9b030>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] Tensor object at 0x7fd070f0f970>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070f0f910>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070ebe7b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] NDArray object at 0x7fd070f0f850>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 32\n",
      "        new_strides = [36, -12, -4, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070ebe7b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] NDArray object at 0x7fd070f0f850>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070ebe7b0>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070ebe230>, (2, 3, 3, 4), (36, -12, -4, 1), 32\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[ 0.67229474  0.40746182 -0.76991606  0.5392492 ]\n",
      "   [-0.6743327   0.03183056 -0.6358461   0.67643327]\n",
      "   [...0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.        ]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070ebe7b0>, (2, 3, 3, 4), (36, -12, -4, 1), 32') raised in repr()] NDArray object at 0x7fd070f0f850>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cpu()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b4f370>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd060b4f370>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070d587b0>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] Tensor object at 0x7fd060b4fb50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070d58930>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] Tensor object at 0x7fd060b4fb50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd060b4f370>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070ef0170>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] NDArray object at 0x7fd060b4f2b0>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 68\n",
      "        new_strides = [72, -24, -4, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070ef0170>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] NDArray object at 0x7fd060b4f2b0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070ef0170>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070ef06b0>, (3, 3, 6, 4), (72, -24, -4, 1), 68\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[5.28289521e-43 1.12103877e-44 1.40129846e-45 4.58504858e-41]\n",
      "   [1.10361272e+20 4.58504858e-41 5.35296013e...6e-44 0.00000000e+00            nan]\n",
      "   [0.00000000e+00 0.00000000e+00 5.61920684e-43 3.64337601e-44]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070ef0170>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] NDArray object at 0x7fd060b4f2b0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (1, 2)\n",
      "device     = cuda()\n",
      "params     = {'axes': (1, 2), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (1, 2)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070ee0430>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070ee0430>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd0fd54cf30>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] Tensor object at 0x7fd070ee08b0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd06803ae30>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] Tensor object at 0x7fd070ee08b0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070ee0430>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "        axes       = (1, 2)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070d44430>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] NDArray object at 0x7fd070ee09d0>\n",
      "        axes       = (1, 2)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 68\n",
      "        new_strides = [72, -24, -4, 1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070d44430>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] NDArray object at 0x7fd070ee09d0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d44430>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070d44bf0>, (3, 3, 6, 4), (72, -24, -4, 1), 68\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]\n",
      "\n",
      "  [[0.... [[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070d44430>, (3, 3, 6, 4), (72, -24, -4, 1), 68') raised in repr()] NDArray object at 0x7fd070ee09d0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e65850>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e65850>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd068041af0>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] Tensor object at 0x7fd070e658e0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd0680412f0>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] Tensor object at 0x7fd070e658e0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e65850>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070d81c30>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] NDArray object at 0x7fd070e65af0>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 11\n",
      "        new_strides = [36, 12, -4, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070d81c30>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] NDArray object at 0x7fd070e65af0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070d81c30>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070d81fb0>, (2, 3, 3, 4), (36, 12, -4, -1), 11\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0.0000000e+00 0.0000000e+00 5.7733497e-43 5.6051939e-44]\n",
      "   [7.7248696e+19 4.5850486e-41 0.0000000e+00 0.0...0000000e+00 1.0990148e+20 4.5850486e-41]\n",
      "   [5.8153886e-43 3.9236357e-44 0.0000000e+00 0.0000000e+00]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070d81c30>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] NDArray object at 0x7fd070e65af0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d36d30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d36d30>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070e03130>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] Tensor object at 0x7fd070d36a60>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070e03eb0>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] Tensor object at 0x7fd070d36a60>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d36d30>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd0fd56f970>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] NDArray object at 0x7fd070d361f0>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 11\n",
      "        new_strides = [36, 12, -4, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd0fd56f970>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] NDArray object at 0x7fd070d361f0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd0fd56f970>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd0fd56fc30>, (2, 3, 3, 4), (36, 12, -4, -1), 11\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[ 0.67229474  0.40746182 -0.76991606  0.5392492 ]\n",
      "   [-0.6743327   0.03183056 -0.6358461   0.67643327]\n",
      "   [...0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.        ]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd0fd56f970>, (2, 3, 3, 4), (36, 12, -4, -1), 11') raised in repr()] NDArray object at 0x7fd070d361f0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd068081df0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd068081df0>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070e07f70>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] Tensor object at 0x7fd068081790>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd070e072b0>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] Tensor object at 0x7fd068081790>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd068081df0>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070e211f0>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] NDArray object at 0x7fd068081130>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 23\n",
      "        new_strides = [72, 24, -4, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070e211f0>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] NDArray object at 0x7fd068081130>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070e211f0>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070e21f30>, (3, 3, 6, 4), (72, 24, -4, -1), 23\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[-3.42734186e+37  4.58504858e-41  8.36575183e-43  3.08285662e-44]\n",
      "   [-3.42276767e+37  4.58504858e-41 -3.40... 8.43581676e-43  5.32493416e-44]\n",
      "   [ 3.91366576e-15  4.58518871e-41  0.00000000e+00  0.00000000e+00]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070e211f0>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] NDArray object at 0x7fd068081130>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (2, 3), 'shape': (3, 3, 6, 4)}\n",
      "shape      = (3, 3, 6, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7380309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]])\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d4ea30>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n",
      "   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070d4ea30>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd068026070>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] Tensor object at 0x7fd070d4ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...rray object at 0x7fd068026d70>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] Tensor object at 0x7fd070d4ee50>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070d4ea30>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "        axes       = (2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070e49470>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] NDArray object at 0x7fd070d4e2e0>\n",
      "        axes       = (2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 23\n",
      "        new_strides = [72, 24, -4, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.2799246  -0.09815039  0.9101789   0.3172182 ]\n",
      "   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070e49470>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] NDArray object at 0x7fd070d4e2e0>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070e49470>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070e49670>, (3, 3, 6, 4), (72, 24, -4, -1), 23\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]\n",
      "\n",
      "  [[0.... [[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...ray object at 0x7fd070e49470>, (3, 3, 6, 4), (72, 24, -4, -1), 23') raised in repr()] NDArray object at 0x7fd070d4e2e0>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m____ test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}, device = cpu()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cpu()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1, 2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e71910>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070e71910>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd070ea66b0>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] Tensor object at 0x7fd070e71970>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd070e672b0>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] Tensor object at 0x7fd070e71970>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070e71910>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070dd2e70>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] NDArray object at 0x7fd070e71850>\n",
      "        axes       = (0, 1, 2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 71\n",
      "        new_strides = [-36, -12, -4, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [... 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cpu())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070dd2e70>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] NDArray object at 0x7fd070e71850>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cpu.Array, arg1: needle.backend_ndarray.ndarray_backend_cpu.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070dd2e70>, <needle.backend_ndarray.ndarray_backend_cpu.Array object at 0x7fd070dd2930>, (2, 3, 3, 4), (-36, -12, -4, -1), 71\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[1.08513107e+20 4.58504858e-41 3.02680468e-43 3.92363570e-44]\n",
      "   [1.08374727e+20 4.58504858e-41 1.06657906e...8e-41 0.00000000e+00 0.00000000e+00]\n",
      "   [3.02680468e-43 7.70714155e-44 1.40129846e-45 0.00000000e+00]]]], device=cpu())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070dd2e70>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] NDArray object at 0x7fd070e71850>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[31m\u001b[1m___ test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n",
      "\n",
      "params = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}, device = cuda()\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, flip_backward_params)\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_flip_backward\u001b[39;49;00m(params, device):\n",
      "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\n",
      "        shape, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      ">       backward_check(ndl.flip, ndl.Tensor(np.random.randn(*shape), device=device), axes=axes)\n",
      "\n",
      "axes       = (0, 1, 2, 3)\n",
      "device     = cuda()\n",
      "params     = {'axes': (0, 1, 2, 3), 'shape': (2, 3, 3, 4)}\n",
      "shape      = (2, 3, 3, 4)\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:145: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/test_conv.py\u001b[0m:16: in backward_check\n",
      "    out = f(*args, **kwargs)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        eps        = 0.001\n",
      "        f          = <function flip at 0x7fd0fd4c1dc0>\n",
      "        kwargs     = {'axes': (0, 1, 2, 3)}\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:563: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m Flip(axes)(a)\n",
      "        a          = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ...7262826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]])\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:73: in __call__\n",
      "    \u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\n",
      "        args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        self       = <needle.ops.Flip object at 0x7fd070ea5400>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:244: in make_from_op\n",
      "    tensor.realize_cached_data()\n",
      "        inputs     = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n",
      "   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n",
      "        op         = <needle.ops.Flip object at 0x7fd070ea5400>\n",
      "        tensor     = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd06806fc30>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] Tensor object at 0x7fd070ea54f0>\n",
      "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:100: in realize_cached_data\n",
      "    \u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\n",
      "        self       = <[TypeError('compact(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: ne...ay object at 0x7fd060b07570>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] Tensor object at 0x7fd070ea54f0>\n",
      "\u001b[1m\u001b[31mpython/needle/ops.py\u001b[0m:553: in compute\n",
      "    \u001b[94mreturn\u001b[39;49;00m array_api.flip(a, \u001b[96mself\u001b[39;49;00m.axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "        self       = <needle.ops.Flip object at 0x7fd070ea5400>\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:704: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m a.flip(axes)\n",
      "        a          = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "        axes       = (0, 1, 2, 3)\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:629: in flip\n",
      "    \u001b[94mreturn\u001b[39;49;00m arr.compact()\n",
      "        arr        = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070ef9230>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] NDArray object at 0x7fd070ea5640>\n",
      "        axes       = (0, 1, 2, 3)\n",
      "        i          = 3\n",
      "        n          = 4\n",
      "        new_offset = 71\n",
      "        new_strides = [-36, -12, -4, -1]\n",
      "        self       = NDArray([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n",
      "   [ 1.867558   -0.9772779   0.95008844 -0.1513572 ]\n",
      "   [...0.17742614 -0.40178093 -1.6301984   0.46278226]\n",
      "   [-0.9072984   0.0519454   0.7290906   0.12898292]]]], device=cuda())\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070ef9230>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] NDArray object at 0x7fd070ea5640>\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mcompact\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Convert a matrix to be compact\"\"\"\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.is_compact():\n",
      "            \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m\n",
      "        \u001b[94melse\u001b[39;49;00m:\n",
      "            out = NDArray.make(\u001b[96mself\u001b[39;49;00m.shape, device=\u001b[96mself\u001b[39;49;00m.device)\n",
      ">           \u001b[96mself\u001b[39;49;00m.device.compact(\n",
      "                \u001b[96mself\u001b[39;49;00m._handle, out._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\n",
      "            )\n",
      "\u001b[1m\u001b[31mE           TypeError: compact(): incompatible function arguments. The following argument types are supported:\u001b[0m\n",
      "\u001b[1m\u001b[31mE               1. (arg0: needle.backend_ndarray.ndarray_backend_cuda.Array, arg1: needle.backend_ndarray.ndarray_backend_cuda.Array, arg2: List[int], arg3: List[int], arg4: int) -> None\u001b[0m\n",
      "\u001b[1m\u001b[31mE           \u001b[0m\n",
      "\u001b[1m\u001b[31mE           Invoked with: <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070ef9230>, <needle.backend_ndarray.ndarray_backend_cuda.Array object at 0x7fd070ef9f70>, (2, 3, 3, 4), (-36, -12, -4, -1), 71\u001b[0m\n",
      "\n",
      "out        = NDArray([[[[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]\n",
      "\n",
      "  [[...[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0.]]]], device=cuda())\n",
      "self       = <[TypeError('to_numpy(): incompatible function arguments. The following argument types are supported:\\n    1. (arg0: n...y object at 0x7fd070ef9230>, (2, 3, 3, 4), (-36, -12, -4, -1), 71') raised in repr()] NDArray object at 0x7fd070ea5640>\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:215: TypeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31mFAILED\u001b[0m tests/test_conv.py::\u001b[1mtest_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: compact(): incompatible function arguments. The following argume...\n",
      "\u001b[31m===================== \u001b[31m\u001b[1m40 failed\u001b[0m, \u001b[33m1763 deselected\u001b[0m\u001b[31m in 6.14s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "\\Longrightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "3 & 0 & 4 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n",
    "\n",
    "\n",
    "Implement `Dilate` in `ops.py`. This function takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"dilate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit new ops (flip/dilation) to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"new_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution forward\n",
    "\n",
    "Implement the forward pass of 2D multi-channel convolution in `ops.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n",
    "\n",
    "**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n",
    "\n",
    "However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2). \n",
    "\n",
    "Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n",
    "\n",
    "We recommend implementing convolution without stride first, ensuring you pass some of the tests below, and then adding in stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n",
    "\n",
    "Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n",
    "\n",
    "In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n",
    "\n",
    "`X.grad = out_grad @ W.transpose` \\\n",
    "`W.grad = X.transpose @ out_grad`\n",
    "\n",
    "Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n",
    "\n",
    "`X.grad = ≈conv(≈out_grad, ≈W)` \\\n",
    "`W.grad = ≈conv(≈X, ≈out_grad)`\n",
    "\n",
    "In which the \"≈\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n",
    "\n",
    "As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n",
    "\n",
    "Summarizing some hints for both `X.grad` and `W.grad`:\n",
    "\n",
    "`X.grad`\n",
    "- The convolution of `out_grad` and `W`, with some operations applied to those\n",
    "- `W` should be flipped over both the kernel dimensions\n",
    "- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n",
    "- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape \n",
    "    - This padding depends on both the kernel size and the `padding` argument to the convolution\n",
    "\n",
    "`W.grad`\n",
    "- The convolution of `X` and `out_grad`, with some operations applied to those\n",
    "- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n",
    "    - Consider turning batches into channels via transpose/permute\n",
    "- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n",
    "    - Remember to account for the `padding` argument passed to convolution\n",
    "\n",
    "General tips\n",
    "- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n",
    "- Start with the case where `padding=0`, then consider changing `padding` arguments\n",
    "- You can \"permute\" axes with multiple calls to `transpose`\n",
    "\n",
    "It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"op_conv and backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Fixing init._calculate_fans for convolution\n",
    "Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n",
    "For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n",
    "\n",
    "**You will need to edit your `kaiming_uniform`, etc. init functions to support multidimensional arrays.** In particular, you should add a new `shape` argument which is then passed to, e.g., the underlying `rand` function.\n",
    "\n",
    "You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"kaiming_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing nn.Conv\n",
    "\n",
    "Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n",
    "which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways). \n",
    "\n",
    "Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n",
    "\n",
    "- Ensure nn.Conv works for (N, C, H, W) tensors even though we implemented the conv op for (N, H, W, C) tensors\n",
    "- Initialize the (k, k, i, o) weight tensor using Kaiming uniform initialization with default settings\n",
    "- Initialize the (o,) bias tensor using uniform initialization on the interval $\\pm$`1.0/(in_channels * kernel_size**2)**0.5`\n",
    "- Calculate the appropriate padding to ensure input and output dimensions are the same\n",
    "- Calculate the convolution, then add the properly-broadcasted bias term if present\n",
    "\n",
    "You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"nn_conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit nn.Conv to mugrade [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"conv_backward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementing \"ResNet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n",
    "\n",
    "In the figure below, before the linear layer, you should \"flatten\" the tensor. We have added a module called `Flatten` in `nn.py` that you can complete and use, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n",
    "\n",
    "Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n",
    "\n",
    "<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n",
    "\n",
    "We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit ResNet9 to mugrade [10 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"resnet9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "from models import ResNet9\n",
    "from simple_training import train_cifar10, evaluate_cifar10\n",
    "\n",
    "device = ndl.cpu()\n",
    "dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n",
    "dataloader = ndl.data.DataLoader(\\\n",
    "         dataset=dataset,\n",
    "         batch_size=128,\n",
    "         shuffle=True,\n",
    "         collate_fn=ndl.data.collate_ndarray,\n",
    "         device=device,\n",
    "         dtype=\"float32\")\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n",
    "      lr=0.001, weight_decay=0.001)\n",
    "evaluate_cifar10(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Recurrent neural network [10 points]\n",
    "\n",
    "**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNNCell`.\n",
    "\n",
    "$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "In `python/needle/nn.py`, implement `RNN`.\n",
    "\n",
    "For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n",
    "\n",
    "where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n",
    "\n",
    "In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Long short-term memory network [10 points]\n",
    "Implement - `Sigmoid`\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$\n",
    "\n",
    "In `python/needle/nn.py`, implement `Sigmoid`, `LSTMCell` and `LSTM`.\n",
    "\n",
    "\\begin{align}\n",
    "i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n",
    "f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n",
    "g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n",
    "o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n",
    "c^\\prime &= f * c + i * g \\\\\n",
    "h^\\prime &= o * \\text{tanh}(c^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively. \n",
    "\n",
    "All weights and biases should be initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{hidden_size}}$.\n",
    "\n",
    "Now implement `LSTM` in `python/needle/nn.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n",
    "\n",
    "\\begin{align}\n",
    "i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n",
    "f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n",
    "g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n",
    "o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n",
    "c_t &= f * c_{(t-1)} + i * g \\\\\n",
    "h_t &= o * \\text{tanh}(c_t)\n",
    "\\end{align},\n",
    "where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively. \n",
    "\n",
    "In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"test_lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Penn Treebank dataset [10 points]\n",
    "\n",
    "In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n",
    "\n",
    "In `python/needle/data.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n",
    "\n",
    "Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n",
    "\n",
    "In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "\n",
    "```\n",
    "┌ a g m s ┐\n",
    "│ b h n t │\n",
    "│ c i o u │\n",
    "│ d j p v │\n",
    "│ e k q w │\n",
    "└ f l r x ┘\n",
    "```\n",
    "\n",
    "These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n",
    "\n",
    "Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "```\n",
    "┌ a g m s ┐ ┌ b h n t ┐\n",
    "└ b h n t ┘ └ c i o u ┘\n",
    "```\n",
    "Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"ptb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ptb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training a word-level language model [10 points]\n",
    "\n",
    "Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n",
    "\n",
    "First, in `python/needle/nn.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n",
    "\n",
    "In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of \n",
    "\n",
    "- An embedding layer (which maps word IDs to embeddings) \n",
    "- A sequence model (either RNN or LSTM)\n",
    "- A linear layer (which outputs probabilities of the next word)\n",
    "\n",
    "In `apps/simple_training.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_implementation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -l -v -k \"language_model_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"language_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your language model on the Penn Treebank dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import needle as ndl\n",
    "sys.path.append('./apps')\n",
    "from models import LanguageModel\n",
    "from simple_training import train_ptb, evaluate_ptb\n",
    "\n",
    "device = ndl.cpu()\n",
    "corpus = ndl.data.Corpus(\"data/ptb\")\n",
    "train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n",
    "model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n",
    "train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n",
    "evaluate_ptb(model, train_data, seq_len=40, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
